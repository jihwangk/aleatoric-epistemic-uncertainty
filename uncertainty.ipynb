{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "uncertainty.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bc34af37197d41408d637781aea9088d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5504e28aaa6f46ec8e03338a65bc65a4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5c0ea330f1f749628908c60720002d84",
              "IPY_MODEL_f57b73b62dcd4dccad14c95a6d8fd69a"
            ]
          }
        },
        "5504e28aaa6f46ec8e03338a65bc65a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c0ea330f1f749628908c60720002d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b10abc998f4b4e29a077918c8740b335",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3db47edb582145f7956ef0cc44e16f37"
          }
        },
        "f57b73b62dcd4dccad14c95a6d8fd69a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_087713df0ef842cd89d2dceb3cf6386d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:30&lt;00:00, 16204879.69it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f108da7bdff2421cbb94aca925b84072"
          }
        },
        "b10abc998f4b4e29a077918c8740b335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3db47edb582145f7956ef0cc44e16f37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "087713df0ef842cd89d2dceb3cf6386d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f108da7bdff2421cbb94aca925b84072": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P7rECAWICWf",
        "colab_type": "text"
      },
      "source": [
        "#Setup Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhxDg0UNrQSO",
        "colab_type": "text"
      },
      "source": [
        "Install tensorboard logger for logging and mount my drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OJZ9zo67Tk3",
        "colab_type": "code",
        "outputId": "33581aae-2800-4246-899b-946b5312256c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "!pip install tensorboard_logger\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboard_logger\n",
            "  Downloading https://files.pythonhosted.org/packages/87/7a/ec0fd26dba69191f82eb8f38f5b401c124f45a207490a7ade6ea9717ecdb/tensorboard_logger-0.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from tensorboard_logger) (3.10.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard_logger) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboard_logger) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard_logger) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboard_logger) (1.18.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->tensorboard_logger) (46.1.3)\n",
            "Installing collected packages: tensorboard-logger\n",
            "Successfully installed tensorboard-logger-0.1.0\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL00gm21RHlW",
        "colab_type": "text"
      },
      "source": [
        "Manage imports here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWQPkV83RAav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import torch.optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributions as dist\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from tensorboard_logger import configure, log_value\n",
        "\n",
        "# import torch.nn.parallel\n",
        "# import torch.distributed as dist\n",
        "# import torch.utils.data.distributed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHVVYKIrIS-h",
        "colab_type": "text"
      },
      "source": [
        "#Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POxogOxT8Czv",
        "colab_type": "text"
      },
      "source": [
        "Implement custom loss functions here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDQzI8TyITee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class AleatoricLoss(nn.Module):\n",
        "#     def __init__(self, T_samples=1):\n",
        "#         super(AleatoricLoss, self).__init__()\n",
        "#         self.T_samples = T_samples\n",
        "\n",
        "#     def forward(self, input, target, model):\n",
        "#         probs = []\n",
        "\n",
        "#         # For t from 1 ~ T, do:\n",
        "#         for i in range(self.T_samples):\n",
        "#             # Get f^(W_hat_t)(x) from putting input to the model with dropout\n",
        "#             output = model.forward(input)\n",
        "\n",
        "#             # Compute softmax on the output per image\n",
        "#             output = F.softmax(output, dim=1)\n",
        "\n",
        "#             # Add the batch to the probabilities array\n",
        "#             probs.append(output)\n",
        "\n",
        "#         # Compute the mean of probabilities\n",
        "#         probs = torch.stack(probs, dim=0)\n",
        "#         mean = probs.mean(dim=0)\n",
        "\n",
        "#         # Sample prediction from probability distribution\n",
        "#         prob_dist = dist.categorical.Categorical(mean)\n",
        "#         sampled = prob_dist.sample()\n",
        "\n",
        "#         # Compare prediction to target and compute mean squared error\n",
        "#         ones = torch.ones_like(sampled, dtype=torch.float).cuda()\n",
        "#         correct = (sampled == target).type(torch.FloatTensor).cuda()\n",
        "#         return F.mse_loss(correct, ones)\n",
        "\n",
        "class EpistemicLoss(nn.Module):\n",
        "    def __init__(self, T_samples=1):\n",
        "        super(EpistemicLoss, self).__init__()\n",
        "        self.T_samples = T_samples\n",
        "\n",
        "    def forward(self, input, target, model):\n",
        "        probs = []\n",
        "\n",
        "        # For t from 1 ~ T, do:\n",
        "        for i in range(self.T_samples):\n",
        "            # Get f^(W_hat_t)(x) from putting input to the model with dropout\n",
        "            output = model.forward(input)\n",
        "\n",
        "            # Compute softmax on the output per image\n",
        "            output = F.softmax(output, dim=1)\n",
        "\n",
        "            # Add the batch to the probabilities array\n",
        "            probs.append(output)\n",
        "\n",
        "        # Compute the mean of probabilities\n",
        "        probs = torch.stack(probs, dim=0)\n",
        "        mean = probs.mean(dim=0)\n",
        "\n",
        "        # Sample prediction from probability distribution\n",
        "        prob_dist = dist.categorical.Categorical(mean)\n",
        "        sampled = prob_dist.sample()\n",
        "\n",
        "        # Compare prediction to target and compute mean squared error\n",
        "        ones = torch.ones_like(sampled, dtype=torch.float).cuda()\n",
        "        correct = (sampled == target).type(torch.FloatTensor).cuda()\n",
        "        return F.mse_loss(correct, ones)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt0NBC4CNhZQ",
        "colab_type": "text"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uldo7aGKQvCK",
        "colab_type": "text"
      },
      "source": [
        "We will use PyramidNet (https://github.com/dyhan0920/PyramidNet-PyTorch) for our model. The code was copied and modified instead of cloned due to necessary modification to the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmMBT2GeNj7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    outchannel_ratio = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)        \n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn3 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(x)\n",
        "        out = self.conv1(out)        \n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn3(out)\n",
        "       \n",
        "        if self.downsample is not None:\n",
        "            shortcut = self.downsample(x)\n",
        "            featuremap_size = shortcut.size()[2:4]\n",
        "        else:\n",
        "            shortcut = x\n",
        "            featuremap_size = out.size()[2:4]\n",
        "\n",
        "        batch_size = out.size()[0]\n",
        "        residual_channel = out.size()[1]\n",
        "        shortcut_channel = shortcut.size()[1]\n",
        "\n",
        "        if residual_channel != shortcut_channel:\n",
        "            padding = torch.autograd.Variable(torch.cuda.FloatTensor(batch_size, residual_channel - shortcut_channel, \n",
        "                                                                     featuremap_size[0], featuremap_size[1]).fill_(0)) \n",
        "            out += torch.cat((shortcut, padding), 1)\n",
        "        else:\n",
        "            out += shortcut \n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    outchannel_ratio = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, (planes*1), kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d((planes*1))\n",
        "        self.conv3 = nn.Conv2d((planes*1), planes * Bottleneck.outchannel_ratio, kernel_size=1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(planes * Bottleneck.outchannel_ratio)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(x)\n",
        "        out = self.conv1(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn3(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn4(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            shortcut = self.downsample(x)\n",
        "            featuremap_size = shortcut.size()[2:4]\n",
        "        else:\n",
        "            shortcut = x\n",
        "            featuremap_size = out.size()[2:4]\n",
        "\n",
        "        batch_size = out.size()[0]\n",
        "        residual_channel = out.size()[1]\n",
        "        shortcut_channel = shortcut.size()[1]\n",
        "\n",
        "        if residual_channel != shortcut_channel:\n",
        "            padding = torch.autograd.Variable(torch.cuda.FloatTensor(batch_size, residual_channel - shortcut_channel, \n",
        "                                                                     featuremap_size[0], featuremap_size[1]).fill_(0)) \n",
        "            out += torch.cat((shortcut, padding), 1)\n",
        "        else:\n",
        "            out += shortcut \n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class PyramidNet(nn.Module):\n",
        "    def __init__(self, dataset, depth, alpha, num_classes, bottleneck=False, dropout_rate=0.5):\n",
        "        super(PyramidNet, self).__init__()   \t\n",
        "        self.dataset = dataset\n",
        "        self.inplanes = 16\n",
        "        if bottleneck == True:\n",
        "            n = int((depth - 2) / 9)\n",
        "            block = Bottleneck\n",
        "        else:\n",
        "            n = int((depth - 2) / 6)\n",
        "            block = BasicBlock\n",
        "\n",
        "        self.addrate = alpha / (3*n*1.0)\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.input_featuremap_dim = self.inplanes\n",
        "        self.conv1 = nn.Conv2d(3, self.input_featuremap_dim, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.input_featuremap_dim)\n",
        "\n",
        "        self.featuremap_dim = self.input_featuremap_dim \n",
        "        self.layer1 = self.pyramidal_make_layer(block, n)\n",
        "        self.layer2 = self.pyramidal_make_layer(block, n, stride=2)\n",
        "        self.layer3 = self.pyramidal_make_layer(block, n, stride=2)\n",
        "\n",
        "        self.final_featuremap_dim = self.input_featuremap_dim\n",
        "        self.bn_final= nn.BatchNorm2d(self.final_featuremap_dim)\n",
        "        self.relu_final = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(self.final_featuremap_dim, num_classes)\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def pyramidal_make_layer(self, block, block_depth, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1:\n",
        "            downsample = nn.AvgPool2d((2,2), stride = (2, 2), ceil_mode=True)\n",
        "\n",
        "        layers = []\n",
        "        self.featuremap_dim = self.featuremap_dim + self.addrate\n",
        "        layers.append(block(self.input_featuremap_dim, int(round(self.featuremap_dim)), stride, downsample))\n",
        "        for i in range(1, block_depth):\n",
        "            temp_featuremap_dim = self.featuremap_dim + self.addrate\n",
        "            layers.append(block(int(round(self.featuremap_dim)) * block.outchannel_ratio, int(round(temp_featuremap_dim)), 1))\n",
        "            self.featuremap_dim  = temp_featuremap_dim\n",
        "        self.input_featuremap_dim = int(round(self.featuremap_dim)) * block.outchannel_ratio\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.bn_final(x)\n",
        "        x = self.relu_final(x)\n",
        "        x = F.dropout(x, p=self.dropout_rate)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "    \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm3YjxQlVa70",
        "colab_type": "text"
      },
      "source": [
        "Initialize config variables needed for the model. We will use a recommended config, which is \"PyramidNet-110 (alpha=48 without bottleneck) on CIFAR-10 dataset with a single-GPU\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBedL-B5TajP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_err1 = 100\n",
        "best_err5 = 100\n",
        "\n",
        "augment = True # Whether to use standard augmentation for CIFAR dataset\n",
        "dataset_name = \"cifar10\" # Specify whether to use CIFAR-10 or CIFAR-100\n",
        "loss_func = \"epistemic\" # Specify which loss function to use\n",
        "# loss_func = \"cross_entropy\" # Uncomment this and comment upper line if using cross entropy loss\n",
        "batch_size = 32 # Mini-batch size\n",
        "workers = 4 # Number of data loading workers\n",
        "depth = 110 # Depth of the network\n",
        "alpha = 64 # Number of new channel increases per depth\n",
        "bottleneck = False # Whether to use bottleneck or basicblock\n",
        "learning_rate = 0.1 # Initial learning rate\n",
        "momentum = 0.9 # Momentum value for SGD\n",
        "weight_decay = 1e-4 # Weight decay for SGD\n",
        "resume = None # Path to resume from if restarting\n",
        "start_epoch = 1 # Manual epoch number in case of restarts\n",
        "epochs = 5 # Total epochs to run\n",
        "print_freq = 25 # Print frequency\n",
        "verbose = True # Whether to print per frequency\n",
        "T_samples = 10 # T sampled masked model weights to use\n",
        "dropout_rate = 0.25 # Dropout rate for epistemic loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxPevkiNVhTW",
        "colab_type": "text"
      },
      "source": [
        "Define functions to train and validate the model, copied and modified from the repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqSc7-iDUYQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_to_pickle(dataset, fileloc):\n",
        "    filename = fileloc + \".pickle\"\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(dataset, f)\n",
        "    print(\"Saved to {}\".format(filename))\n",
        "\n",
        "def load_from_pickle(fileloc):\n",
        "    filename = fileloc + \".pickle\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        dataset = pickle.load(f)\n",
        "    print(\"Loaded from {}\".format(filename))\n",
        "    return dataset\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, loss_func):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    current_LR = get_learning_rate(optimizer)[0]\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        target = target.cuda(async=True)\n",
        "        input_var = torch.autograd.Variable(input)\n",
        "        target_var = torch.autograd.Variable(target)\n",
        "\n",
        "        # compute output\n",
        "        output = model(input_var)\n",
        "        if loss_func == \"epistemic\":\n",
        "            loss = criterion(input_var, target_var, model)\n",
        "        else:\n",
        "            loss = criterion(output, target_var)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        err1, err5 = accuracy(output.data, target, topk=(1, 5))\n",
        "        losses.update(loss.data.item(), input.size(0))\n",
        "        top1.update(err1[0], input.size(0))\n",
        "        top5.update(err5[0], input.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss = torch.autograd.Variable(loss, requires_grad = True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % print_freq == 0 and verbose == True:\n",
        "            print('Epoch: [{0}/{1}][{2}/{3}]\\t'\n",
        "                  'LR: {LR:.6f}\\t' \n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Top 1-err {top1.val:.4f} ({top1.avg:.4f})\\t'\n",
        "                  'Top 5-err {top5.val:.4f} ({top5.avg:.4f})'.format(\n",
        "                   epoch, epochs, i, len(train_loader), LR=current_LR, batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
        "    log_value('train_loss', losses.avg, epoch)\n",
        "    log_value('train_error', top1.avg, epoch)\n",
        "\n",
        "def validate(val_loader, model, criterion, epoch, loss_func):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    for i, (input, target) in enumerate(val_loader):\n",
        "        target = target.cuda(async=True)\n",
        "\n",
        "        # for PyTorch 0.4.x, volatile=True is replaced by with torch.no.grad(), so uncomment the followings:\n",
        "        with torch.no_grad():\n",
        "            input_var = torch.autograd.Variable(input)\n",
        "            target_var = torch.autograd.Variable(target)\n",
        "            output = model(input_var)\n",
        "            if loss_func == \"epistemic\":\n",
        "                loss = criterion(input_var, target_var, model)\n",
        "            else:\n",
        "                loss = criterion(output, target_var)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        err1, err5 = accuracy(output.data, target, topk=(1, 5))\n",
        "        losses.update(loss.data.item(), input.size(0))\n",
        "        top1.update(err1[0], input.size(0))\n",
        "        top5.update(err5[0], input.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % print_freq == 0 and verbose == True:\n",
        "            print('Test (on val set): [{0}/{1}][{2}/{3}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Top 1-err {top1.val:.4f} ({top1.avg:.4f})\\t'\n",
        "                  'Top 5-err {top5.val:.4f} ({top5.avg:.4f})'.format(\n",
        "                  epoch, epochs, i, len(val_loader), batch_time=batch_time, loss=losses,\n",
        "                  top1=top1, top5=top5))\n",
        "\n",
        "    print('* Epoch: [{0}/{1}]\\t Top 1-err {top1.avg:.3f}  Top 5-err {top5.avg:.3f}\\t Test Loss {loss.avg:.3f}'.format(epoch, epochs, top1=top1, top5=top5, loss=losses))\n",
        "    log_value('val_loss', losses.avg, epoch)\n",
        "    log_value('val_acc', top1.avg, epoch)\n",
        "    return top1.avg, top5.avg\n",
        "\n",
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    directory = \"/../../../content/gdrive/My Drive/PyramidNet/\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    filename = directory + filename\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, directory + 'model_best.pth.tar')\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    lr = learning_rate * (0.1 ** (epoch // (epochs*0.5))) * (0.1 ** (epoch // (epochs*0.75)))\n",
        "    \n",
        "    log_value('learning_rate', lr, epoch)\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def get_learning_rate(optimizer):\n",
        "    lr=[]\n",
        "    for param_group in optimizer.param_groups:\n",
        "       lr +=[ param_group['lr'] ]\n",
        "    return lr\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "        wrong_k = batch_size - correct_k\n",
        "        res.append(wrong_k.mul_(100.0 / batch_size))\n",
        "\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSB942DXVoN-",
        "colab_type": "text"
      },
      "source": [
        "Initialize the model, copied and modified from the repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGnuSsj8UdVd",
        "colab_type": "code",
        "outputId": "3bd8c6fc-0007-42c5-c421-9143d0122953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bc34af37197d41408d637781aea9088d",
            "5504e28aaa6f46ec8e03338a65bc65a4",
            "5c0ea330f1f749628908c60720002d84",
            "f57b73b62dcd4dccad14c95a6d8fd69a",
            "b10abc998f4b4e29a077918c8740b335",
            "3db47edb582145f7956ef0cc44e16f37",
            "087713df0ef842cd89d2dceb3cf6386d",
            "f108da7bdff2421cbb94aca925b84072"
          ]
        }
      },
      "source": [
        "configure(\"/../../../content/gdrive/My Drive/PyramidNet\")\n",
        "# torch.cuda.empty_cache()\n",
        "normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                  std=[x/255.0 for x in [63.0, 62.1, 66.7]])    \n",
        "if augment:\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "        ])\n",
        "else:\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "        ])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "    ])\n",
        "\n",
        "if dataset_name == 'cifar100':\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR100('../data', train=True, download=True, transform=transform_train),\n",
        "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR100('../data', train=False, transform=transform_test),\n",
        "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True) \n",
        "    numberofclass = 100         \n",
        "elif dataset_name == 'cifar10':\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10('../data', train=True, download=True, transform=transform_train),\n",
        "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10('../data', train=False, transform=transform_test),\n",
        "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "    numberofclass = 10\n",
        "else: \n",
        "    raise Exception ('Wrong dataset input: {}'.format(dataset_name)) \n",
        "\n",
        "print(\"=> creating model '{}'\".format(\"PyramidNet\"))\n",
        "model = PyramidNet(dataset_name, depth, alpha, numberofclass, bottleneck=bottleneck, dropout_rate=dropout_rate)\n",
        "model = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "print(model)\n",
        "print('the number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n",
        "\n",
        "# define loss function (criterion) and optimizer\n",
        "if loss_func == \"epistemic\":\n",
        "    criterion = EpistemicLoss(T_samples=T_samples).cuda()\n",
        "else:\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), learning_rate, momentum=momentum, \n",
        "                            weight_decay=weight_decay, nesterov=True)\n",
        "\n",
        "# optionally resume from a checkpoint\n",
        "# if resume:\n",
        "#     if os.path.isfile(resume):\n",
        "#         print(\"=> loading checkpoint '{}'\".format(resume))\n",
        "#         checkpoint = torch.load(resume)\n",
        "#         start_epoch = checkpoint['epoch']\n",
        "#         best_err1 = checkpoint['best_err1']\n",
        "#         best_err5 = checkpoint['best_err5']            \n",
        "#         model.load_state_dict(checkpoint['state_dict'])\n",
        "#         optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "#         print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "#               .format(resume, checkpoint['epoch']))\n",
        "#     else:\n",
        "#         print(\"=> no checkpoint found at '{}'\".format(resume))\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc34af37197d41408d637781aea9088d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/cifar-10-python.tar.gz to ../data\n",
            "=> creating model 'PyramidNet'\n",
            "DataParallel(\n",
            "  (module): PyramidNet(\n",
            "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(16, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(17, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(17, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(18, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): BasicBlock(\n",
            "        (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(20, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(21, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): BasicBlock(\n",
            "        (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(21, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): BasicBlock(\n",
            "        (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(22, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (6): BasicBlock(\n",
            "        (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(23, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (7): BasicBlock(\n",
            "        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(24, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (8): BasicBlock(\n",
            "        (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(25, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (9): BasicBlock(\n",
            "        (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(27, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(28, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (10): BasicBlock(\n",
            "        (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(28, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(29, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (11): BasicBlock(\n",
            "        (bn1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(29, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (12): BasicBlock(\n",
            "        (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(30, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (13): BasicBlock(\n",
            "        (bn1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(31, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(33, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (14): BasicBlock(\n",
            "        (bn1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(33, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(34, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (15): BasicBlock(\n",
            "        (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(34, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(35, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (16): BasicBlock(\n",
            "        (bn1): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(35, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (17): BasicBlock(\n",
            "        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(36, 37, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(37, 37, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (bn1): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(37, 39, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(39, 39, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (bn1): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(39, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(40, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(41, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): BasicBlock(\n",
            "        (bn1): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(41, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(42, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): BasicBlock(\n",
            "        (bn1): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(42, 43, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(43, 43, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): BasicBlock(\n",
            "        (bn1): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(43, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (6): BasicBlock(\n",
            "        (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(44, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(46, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (7): BasicBlock(\n",
            "        (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(46, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(47, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (8): BasicBlock(\n",
            "        (bn1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(47, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (9): BasicBlock(\n",
            "        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(48, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(49, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (10): BasicBlock(\n",
            "        (bn1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(49, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (11): BasicBlock(\n",
            "        (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(50, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(52, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (12): BasicBlock(\n",
            "        (bn1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(52, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(53, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (13): BasicBlock(\n",
            "        (bn1): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(53, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(54, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (14): BasicBlock(\n",
            "        (bn1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(54, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (15): BasicBlock(\n",
            "        (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(55, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (16): BasicBlock(\n",
            "        (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(56, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(57, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (17): BasicBlock(\n",
            "        (bn1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(57, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(59, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(59, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(60, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(61, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): BasicBlock(\n",
            "        (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(62, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(63, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): BasicBlock(\n",
            "        (bn1): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(63, 65, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(65, 65, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): BasicBlock(\n",
            "        (bn1): BatchNorm2d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(65, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(66, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (6): BasicBlock(\n",
            "        (bn1): BatchNorm2d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(66, 67, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(67, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(67, 67, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(67, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (7): BasicBlock(\n",
            "        (bn1): BatchNorm2d(67, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(67, 68, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(68, 68, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (8): BasicBlock(\n",
            "        (bn1): BatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(68, 69, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(69, 69, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (9): BasicBlock(\n",
            "        (bn1): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(69, 71, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(71, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(71, 71, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(71, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (10): BasicBlock(\n",
            "        (bn1): BatchNorm2d(71, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(71, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (11): BasicBlock(\n",
            "        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(72, 73, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(73, 73, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (12): BasicBlock(\n",
            "        (bn1): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(73, 74, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(74, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(74, 74, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(74, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (13): BasicBlock(\n",
            "        (bn1): BatchNorm2d(74, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(74, 75, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(75, 75, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (14): BasicBlock(\n",
            "        (bn1): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(75, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(76, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (15): BasicBlock(\n",
            "        (bn1): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(76, 78, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(78, 78, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (16): BasicBlock(\n",
            "        (bn1): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(78, 79, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(79, 79, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (17): BasicBlock(\n",
            "        (bn1): BatchNorm2d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv1): Conv2d(79, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (bn_final): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu_final): ReLU(inplace=True)\n",
            "    (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
            "    (fc): Linear(in_features=80, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "the number of model parameters: 2616376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9uXKSLSVuWV",
        "colab_type": "text"
      },
      "source": [
        "Train and validate the model using custom classification loss function, copied and modified from the repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgZcvrDXfGg2",
        "colab_type": "code",
        "outputId": "afb38a23-3cdb-497d-b785-1d50f1c2ea5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "results = defaultdict(list)\n",
        "\n",
        "for epoch in range(start_epoch, epochs + 1):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "    # train for one epoch\n",
        "    train(train_loader, model, criterion, optimizer, epoch, loss_func)\n",
        "\n",
        "    # evaluate on validation set\n",
        "    err1, err5 = validate(val_loader, model, criterion, epoch, loss_func)\n",
        "\n",
        "    # remember best prec@1 and save checkpoint\n",
        "    is_best = err1 <= best_err1\n",
        "    best_err1 = min(err1, best_err1)\n",
        "    if is_best:\n",
        "        best_err5 = err5\n",
        "    print ('Current best accuracy (top-1 and 5 error):', best_err1, best_err5)\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch,\n",
        "        'arch': \"PyramidNet\",\n",
        "        'state_dict': model.state_dict(),\n",
        "        'best_err1': best_err1,\n",
        "        'best_err5': best_err5,\n",
        "        'optimizer' : optimizer.state_dict(),\n",
        "    }, is_best)\n",
        "\n",
        "    results[\"model\"].append(model)\n",
        "    results[\"epoch\"].append(epoch)\n",
        "    results[\"err1\"].append(err1)\n",
        "    results[\"err5\"].append(err5)\n",
        "    results[\"best_err1\"].append(best_err1)\n",
        "    results[\"best_err5\"].append(best_err5)\n",
        "\n",
        "    save_to_pickle(results, \"/../../../content/gdrive/My Drive/PyramidNet/{}-checkpoint-epoch-{}\".format(loss_func, epoch))\n",
        "\n",
        "print ('Best accuracy (top-1 and 5 error):', best_err1, best_err5)  \n",
        "\n",
        "save_to_pickle(results, \"/../../../content/gdrive/My Drive/PyramidNet/{}-results\".format(loss_func))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of cuda is deprecated:\n",
            "\tcuda(torch.device device, bool async, *, torch.memory_format memory_format)\n",
            "Consider using one of the following signatures instead:\n",
            "\tcuda(torch.device device, bool non_blocking, *, torch.memory_format memory_format)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [1/5][0/1563]\tLR: 0.100000\tTime 1.306 (1.306)\tData 0.334 (0.334)\tLoss 0.8750 (0.8750)\tTop 1-err 96.8750 (96.8750)\tTop 5-err 43.7500 (43.7500)\n",
            "Epoch: [1/5][25/1563]\tLR: 0.100000\tTime 0.358 (0.383)\tData 0.000 (0.013)\tLoss 0.9688 (0.9243)\tTop 1-err 90.6250 (89.0625)\tTop 5-err 59.3750 (46.3942)\n",
            "Epoch: [1/5][50/1563]\tLR: 0.100000\tTime 0.338 (0.364)\tData 0.000 (0.007)\tLoss 0.9375 (0.9130)\tTop 1-err 93.7500 (89.5221)\tTop 5-err 40.6250 (47.0588)\n",
            "Epoch: [1/5][75/1563]\tLR: 0.100000\tTime 0.342 (0.358)\tData 0.000 (0.005)\tLoss 0.9062 (0.9087)\tTop 1-err 96.8750 (89.7615)\tTop 5-err 53.1250 (47.1628)\n",
            "Epoch: [1/5][100/1563]\tLR: 0.100000\tTime 0.342 (0.355)\tData 0.000 (0.003)\tLoss 0.8125 (0.9109)\tTop 1-err 87.5000 (90.3465)\tTop 5-err 46.8750 (48.3292)\n",
            "Epoch: [1/5][125/1563]\tLR: 0.100000\tTime 0.354 (0.354)\tData 0.000 (0.003)\tLoss 0.9062 (0.9080)\tTop 1-err 81.2500 (90.3274)\tTop 5-err 43.7500 (48.5615)\n",
            "Epoch: [1/5][150/1563]\tLR: 0.100000\tTime 0.341 (0.353)\tData 0.000 (0.002)\tLoss 0.9375 (0.9075)\tTop 1-err 87.5000 (90.3767)\tTop 5-err 46.8750 (48.9445)\n",
            "Epoch: [1/5][175/1563]\tLR: 0.100000\tTime 0.336 (0.352)\tData 0.000 (0.002)\tLoss 0.8125 (0.9064)\tTop 1-err 84.3750 (90.3409)\tTop 5-err 43.7500 (48.4553)\n",
            "Epoch: [1/5][200/1563]\tLR: 0.100000\tTime 0.347 (0.351)\tData 0.000 (0.002)\tLoss 0.8125 (0.9069)\tTop 1-err 81.2500 (90.3918)\tTop 5-err 37.5000 (48.2743)\n",
            "Epoch: [1/5][225/1563]\tLR: 0.100000\tTime 0.340 (0.350)\tData 0.000 (0.002)\tLoss 0.9688 (0.9065)\tTop 1-err 93.7500 (90.4729)\tTop 5-err 53.1250 (48.3960)\n",
            "Epoch: [1/5][250/1563]\tLR: 0.100000\tTime 0.345 (0.350)\tData 0.000 (0.001)\tLoss 0.9688 (0.9049)\tTop 1-err 96.8750 (90.3885)\tTop 5-err 59.3750 (48.3068)\n",
            "Epoch: [1/5][275/1563]\tLR: 0.100000\tTime 0.343 (0.349)\tData 0.000 (0.001)\tLoss 0.9062 (0.9048)\tTop 1-err 90.6250 (90.5005)\tTop 5-err 43.7500 (48.5168)\n",
            "Epoch: [1/5][300/1563]\tLR: 0.100000\tTime 0.345 (0.349)\tData 0.000 (0.001)\tLoss 0.9062 (0.9034)\tTop 1-err 100.0000 (90.5835)\tTop 5-err 37.5000 (48.6192)\n",
            "Epoch: [1/5][325/1563]\tLR: 0.100000\tTime 0.347 (0.349)\tData 0.000 (0.001)\tLoss 0.8750 (0.9029)\tTop 1-err 78.1250 (90.5291)\tTop 5-err 56.2500 (48.5717)\n",
            "Epoch: [1/5][350/1563]\tLR: 0.100000\tTime 0.339 (0.348)\tData 0.000 (0.001)\tLoss 0.8125 (0.9030)\tTop 1-err 90.6250 (90.4380)\tTop 5-err 46.8750 (48.3351)\n",
            "Epoch: [1/5][375/1563]\tLR: 0.100000\tTime 0.350 (0.348)\tData 0.000 (0.001)\tLoss 0.9062 (0.9033)\tTop 1-err 78.1250 (90.2344)\tTop 5-err 43.7500 (48.3793)\n",
            "Epoch: [1/5][400/1563]\tLR: 0.100000\tTime 0.343 (0.348)\tData 0.000 (0.001)\tLoss 0.9688 (0.9024)\tTop 1-err 93.7500 (90.2120)\tTop 5-err 43.7500 (48.3713)\n",
            "Epoch: [1/5][425/1563]\tLR: 0.100000\tTime 0.352 (0.348)\tData 0.000 (0.001)\tLoss 0.9375 (0.9012)\tTop 1-err 87.5000 (90.2656)\tTop 5-err 53.1250 (48.4448)\n",
            "Epoch: [1/5][450/1563]\tLR: 0.100000\tTime 0.347 (0.349)\tData 0.000 (0.001)\tLoss 0.9062 (0.9011)\tTop 1-err 90.6250 (90.4171)\tTop 5-err 50.0000 (48.5103)\n",
            "Epoch: [1/5][475/1563]\tLR: 0.100000\tTime 0.338 (0.349)\tData 0.000 (0.001)\tLoss 0.8750 (0.9013)\tTop 1-err 81.2500 (90.3493)\tTop 5-err 50.0000 (48.5754)\n",
            "Epoch: [1/5][500/1563]\tLR: 0.100000\tTime 0.352 (0.349)\tData 0.000 (0.001)\tLoss 0.9062 (0.9003)\tTop 1-err 100.0000 (90.4192)\tTop 5-err 53.1250 (48.7151)\n",
            "Epoch: [1/5][525/1563]\tLR: 0.100000\tTime 0.341 (0.348)\tData 0.000 (0.001)\tLoss 0.8750 (0.9008)\tTop 1-err 96.8750 (90.4171)\tTop 5-err 84.3750 (48.8415)\n",
            "Epoch: [1/5][550/1563]\tLR: 0.100000\tTime 0.339 (0.348)\tData 0.000 (0.001)\tLoss 0.9062 (0.9009)\tTop 1-err 93.7500 (90.4208)\tTop 5-err 53.1250 (48.8544)\n",
            "Epoch: [1/5][575/1563]\tLR: 0.100000\tTime 0.350 (0.348)\tData 0.000 (0.001)\tLoss 0.9062 (0.9014)\tTop 1-err 81.2500 (90.3537)\tTop 5-err 53.1250 (48.7033)\n",
            "Epoch: [1/5][600/1563]\tLR: 0.100000\tTime 0.340 (0.348)\tData 0.000 (0.001)\tLoss 0.8750 (0.9012)\tTop 1-err 93.7500 (90.3702)\tTop 5-err 50.0000 (48.7157)\n",
            "Epoch: [1/5][625/1563]\tLR: 0.100000\tTime 0.335 (0.348)\tData 0.000 (0.001)\tLoss 0.9062 (0.9009)\tTop 1-err 96.8750 (90.4004)\tTop 5-err 43.7500 (48.7220)\n",
            "Epoch: [1/5][650/1563]\tLR: 0.100000\tTime 0.337 (0.348)\tData 0.000 (0.001)\tLoss 0.8125 (0.9007)\tTop 1-err 96.8750 (90.4186)\tTop 5-err 78.1250 (48.9007)\n",
            "Epoch: [1/5][675/1563]\tLR: 0.100000\tTime 0.351 (0.348)\tData 0.000 (0.001)\tLoss 1.0000 (0.9008)\tTop 1-err 93.7500 (90.4817)\tTop 5-err 43.7500 (48.9784)\n",
            "Epoch: [1/5][700/1563]\tLR: 0.100000\tTime 0.348 (0.348)\tData 0.000 (0.001)\tLoss 0.8750 (0.9007)\tTop 1-err 90.6250 (90.4823)\tTop 5-err 46.8750 (49.0460)\n",
            "Epoch: [1/5][725/1563]\tLR: 0.100000\tTime 0.345 (0.348)\tData 0.000 (0.001)\tLoss 0.8750 (0.9005)\tTop 1-err 96.8750 (90.4787)\tTop 5-err 53.1250 (49.0057)\n",
            "Epoch: [1/5][750/1563]\tLR: 0.100000\tTime 0.341 (0.348)\tData 0.000 (0.001)\tLoss 0.9375 (0.9002)\tTop 1-err 96.8750 (90.4419)\tTop 5-err 46.8750 (48.9722)\n",
            "Epoch: [1/5][775/1563]\tLR: 0.100000\tTime 0.350 (0.348)\tData 0.000 (0.001)\tLoss 0.9688 (0.9002)\tTop 1-err 84.3750 (90.4277)\tTop 5-err 40.6250 (48.9369)\n",
            "Epoch: [1/5][800/1563]\tLR: 0.100000\tTime 0.365 (0.348)\tData 0.000 (0.001)\tLoss 1.0000 (0.9004)\tTop 1-err 90.6250 (90.4455)\tTop 5-err 46.8750 (48.8998)\n",
            "Epoch: [1/5][825/1563]\tLR: 0.100000\tTime 0.355 (0.349)\tData 0.000 (0.001)\tLoss 0.8750 (0.9000)\tTop 1-err 96.8750 (90.4926)\tTop 5-err 53.1250 (49.0542)\n",
            "Epoch: [1/5][850/1563]\tLR: 0.100000\tTime 0.342 (0.349)\tData 0.000 (0.001)\tLoss 0.8750 (0.9004)\tTop 1-err 90.6250 (90.5001)\tTop 5-err 40.6250 (49.0232)\n",
            "Epoch: [1/5][875/1563]\tLR: 0.100000\tTime 0.386 (0.349)\tData 0.000 (0.000)\tLoss 0.9062 (0.9014)\tTop 1-err 93.7500 (90.5144)\tTop 5-err 50.0000 (49.0404)\n",
            "Epoch: [1/5][900/1563]\tLR: 0.100000\tTime 0.350 (0.349)\tData 0.000 (0.000)\tLoss 0.9062 (0.9015)\tTop 1-err 93.7500 (90.5418)\tTop 5-err 53.1250 (49.0878)\n",
            "Epoch: [1/5][925/1563]\tLR: 0.100000\tTime 0.340 (0.349)\tData 0.000 (0.000)\tLoss 0.8438 (0.9017)\tTop 1-err 87.5000 (90.5001)\tTop 5-err 40.6250 (49.0213)\n",
            "Epoch: [1/5][950/1563]\tLR: 0.100000\tTime 0.345 (0.349)\tData 0.000 (0.000)\tLoss 0.9375 (0.9019)\tTop 1-err 81.2500 (90.4706)\tTop 5-err 37.5000 (48.9189)\n",
            "Epoch: [1/5][975/1563]\tLR: 0.100000\tTime 0.349 (0.349)\tData 0.000 (0.000)\tLoss 0.9375 (0.9014)\tTop 1-err 90.6250 (90.4969)\tTop 5-err 59.3750 (48.9434)\n",
            "Epoch: [1/5][1000/1563]\tLR: 0.100000\tTime 0.349 (0.349)\tData 0.000 (0.000)\tLoss 0.9062 (0.9013)\tTop 1-err 93.7500 (90.5126)\tTop 5-err 53.1250 (48.9979)\n",
            "Epoch: [1/5][1025/1563]\tLR: 0.100000\tTime 0.366 (0.349)\tData 0.000 (0.000)\tLoss 0.7500 (0.9008)\tTop 1-err 96.8750 (90.5154)\tTop 5-err 50.0000 (48.8731)\n",
            "Epoch: [1/5][1050/1563]\tLR: 0.100000\tTime 0.354 (0.349)\tData 0.000 (0.000)\tLoss 0.8750 (0.9012)\tTop 1-err 90.6250 (90.5209)\tTop 5-err 50.0000 (48.9118)\n",
            "Epoch: [1/5][1075/1563]\tLR: 0.100000\tTime 0.355 (0.349)\tData 0.000 (0.000)\tLoss 0.9375 (0.9014)\tTop 1-err 90.6250 (90.4943)\tTop 5-err 53.1250 (48.9312)\n",
            "Epoch: [1/5][1100/1563]\tLR: 0.100000\tTime 0.354 (0.350)\tData 0.000 (0.000)\tLoss 0.9062 (0.9014)\tTop 1-err 96.8750 (90.5228)\tTop 5-err 40.6250 (48.9243)\n",
            "Epoch: [1/5][1125/1563]\tLR: 0.100000\tTime 0.346 (0.350)\tData 0.000 (0.000)\tLoss 0.9375 (0.9016)\tTop 1-err 81.2500 (90.5140)\tTop 5-err 50.0000 (48.8954)\n",
            "Epoch: [1/5][1150/1563]\tLR: 0.100000\tTime 0.347 (0.350)\tData 0.000 (0.000)\tLoss 0.9062 (0.9019)\tTop 1-err 96.8750 (90.5137)\tTop 5-err 62.5000 (48.8760)\n",
            "Epoch: [1/5][1175/1563]\tLR: 0.100000\tTime 0.360 (0.350)\tData 0.000 (0.000)\tLoss 0.9375 (0.9020)\tTop 1-err 87.5000 (90.5054)\tTop 5-err 28.1250 (48.8839)\n",
            "Epoch: [1/5][1200/1563]\tLR: 0.100000\tTime 0.359 (0.350)\tData 0.000 (0.000)\tLoss 0.9688 (0.9019)\tTop 1-err 93.7500 (90.5339)\tTop 5-err 43.7500 (48.8915)\n",
            "Epoch: [1/5][1225/1563]\tLR: 0.100000\tTime 0.356 (0.350)\tData 0.000 (0.000)\tLoss 0.8438 (0.9017)\tTop 1-err 87.5000 (90.5332)\tTop 5-err 53.1250 (48.9193)\n",
            "Epoch: [1/5][1250/1563]\tLR: 0.100000\tTime 0.345 (0.350)\tData 0.000 (0.000)\tLoss 0.8750 (0.9018)\tTop 1-err 93.7500 (90.5151)\tTop 5-err 40.6250 (48.9059)\n",
            "Epoch: [1/5][1275/1563]\tLR: 0.100000\tTime 0.367 (0.350)\tData 0.000 (0.000)\tLoss 0.9375 (0.9019)\tTop 1-err 87.5000 (90.4903)\tTop 5-err 56.2500 (48.8955)\n",
            "Epoch: [1/5][1300/1563]\tLR: 0.100000\tTime 0.351 (0.350)\tData 0.000 (0.000)\tLoss 0.9375 (0.9017)\tTop 1-err 90.6250 (90.4881)\tTop 5-err 40.6250 (48.8903)\n",
            "Epoch: [1/5][1325/1563]\tLR: 0.100000\tTime 0.363 (0.350)\tData 0.000 (0.000)\tLoss 0.9062 (0.9017)\tTop 1-err 96.8750 (90.4883)\tTop 5-err 56.2500 (48.9607)\n",
            "Epoch: [1/5][1350/1563]\tLR: 0.100000\tTime 0.356 (0.350)\tData 0.000 (0.000)\tLoss 0.8750 (0.9016)\tTop 1-err 81.2500 (90.4261)\tTop 5-err 46.8750 (48.9707)\n",
            "Epoch: [1/5][1375/1563]\tLR: 0.100000\tTime 0.350 (0.351)\tData 0.000 (0.000)\tLoss 0.7812 (0.9017)\tTop 1-err 78.1250 (90.4410)\tTop 5-err 43.7500 (48.9803)\n",
            "Epoch: [1/5][1400/1563]\tLR: 0.100000\tTime 0.353 (0.351)\tData 0.000 (0.000)\tLoss 0.8438 (0.9015)\tTop 1-err 84.3750 (90.4555)\tTop 5-err 40.6250 (48.9739)\n",
            "Epoch: [1/5][1425/1563]\tLR: 0.100000\tTime 0.340 (0.351)\tData 0.000 (0.000)\tLoss 0.9688 (0.9014)\tTop 1-err 93.7500 (90.4738)\tTop 5-err 46.8750 (48.9656)\n",
            "Epoch: [1/5][1450/1563]\tLR: 0.100000\tTime 0.344 (0.351)\tData 0.000 (0.000)\tLoss 0.9375 (0.9014)\tTop 1-err 96.8750 (90.4678)\tTop 5-err 50.0000 (48.9512)\n",
            "Epoch: [1/5][1475/1563]\tLR: 0.100000\tTime 0.355 (0.351)\tData 0.000 (0.000)\tLoss 0.8438 (0.9014)\tTop 1-err 87.5000 (90.4493)\tTop 5-err 43.7500 (48.9266)\n",
            "Epoch: [1/5][1500/1563]\tLR: 0.100000\tTime 0.353 (0.351)\tData 0.000 (0.000)\tLoss 0.8750 (0.9011)\tTop 1-err 93.7500 (90.4522)\tTop 5-err 62.5000 (48.9195)\n",
            "Epoch: [1/5][1525/1563]\tLR: 0.100000\tTime 0.381 (0.351)\tData 0.000 (0.000)\tLoss 0.8438 (0.9008)\tTop 1-err 84.3750 (90.4346)\tTop 5-err 37.5000 (48.8921)\n",
            "Epoch: [1/5][1550/1563]\tLR: 0.100000\tTime 0.342 (0.351)\tData 0.000 (0.000)\tLoss 0.8125 (0.9005)\tTop 1-err 87.5000 (90.4275)\tTop 5-err 46.8750 (48.8677)\n",
            "Test (on val set): [1/5][0/313]\tTime 0.371 (0.371)\tLoss 0.9688 (0.9688)\tTop 1-err 87.5000 (87.5000)\tTop 5-err 62.5000 (62.5000)\n",
            "Test (on val set): [1/5][25/313]\tTime 0.222 (0.227)\tLoss 0.9375 (0.9062)\tTop 1-err 90.6250 (87.7404)\tTop 5-err 56.2500 (49.5192)\n",
            "Test (on val set): [1/5][50/313]\tTime 0.216 (0.223)\tLoss 0.8125 (0.9056)\tTop 1-err 93.7500 (89.2157)\tTop 5-err 37.5000 (50.6740)\n",
            "Test (on val set): [1/5][75/313]\tTime 0.217 (0.222)\tLoss 0.8750 (0.9017)\tTop 1-err 93.7500 (89.9671)\tTop 5-err 50.0000 (49.5066)\n",
            "Test (on val set): [1/5][100/313]\tTime 0.224 (0.222)\tLoss 0.8750 (0.8988)\tTop 1-err 90.6250 (90.0371)\tTop 5-err 50.0000 (48.6077)\n",
            "Test (on val set): [1/5][125/313]\tTime 0.215 (0.221)\tLoss 0.9062 (0.8996)\tTop 1-err 90.6250 (89.6577)\tTop 5-err 50.0000 (48.4623)\n",
            "Test (on val set): [1/5][150/313]\tTime 0.216 (0.221)\tLoss 0.9062 (0.9023)\tTop 1-err 90.6250 (89.8800)\tTop 5-err 50.0000 (48.5513)\n",
            "Test (on val set): [1/5][175/313]\tTime 0.219 (0.221)\tLoss 0.9688 (0.9027)\tTop 1-err 96.8750 (89.6307)\tTop 5-err 53.1250 (48.6151)\n",
            "Test (on val set): [1/5][200/313]\tTime 0.221 (0.221)\tLoss 0.8438 (0.9025)\tTop 1-err 71.8750 (89.5211)\tTop 5-err 28.1250 (48.0721)\n",
            "Test (on val set): [1/5][225/313]\tTime 0.228 (0.221)\tLoss 0.9062 (0.9004)\tTop 1-err 87.5000 (89.6709)\tTop 5-err 53.1250 (47.9674)\n",
            "Test (on val set): [1/5][250/313]\tTime 0.224 (0.221)\tLoss 0.9688 (0.9021)\tTop 1-err 90.6250 (89.7410)\tTop 5-err 43.7500 (47.7714)\n",
            "Test (on val set): [1/5][275/313]\tTime 0.219 (0.221)\tLoss 0.9062 (0.9021)\tTop 1-err 96.8750 (89.9004)\tTop 5-err 43.7500 (47.8374)\n",
            "Test (on val set): [1/5][300/313]\tTime 0.222 (0.221)\tLoss 0.8438 (0.9022)\tTop 1-err 75.0000 (89.9398)\tTop 5-err 28.1250 (47.8717)\n",
            "* Epoch: [1/5]\t Top 1-err 89.940  Top 5-err 47.960\t Test Loss 0.902\n",
            "Current best accuracy (top-1 and 5 error): tensor(89.9400, device='cuda:0') tensor(47.9600, device='cuda:0')\n",
            "Saved to /../../../content/gdrive/My Drive/PyramidNet/epistemic-checkpoint-epoch-1.pickle\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [2/5][0/1563]\tLR: 0.100000\tTime 0.628 (0.628)\tData 0.163 (0.163)\tLoss 0.7188 (0.7188)\tTop 1-err 84.3750 (84.3750)\tTop 5-err 50.0000 (50.0000)\n",
            "Epoch: [2/5][25/1563]\tLR: 0.100000\tTime 0.378 (0.373)\tData 0.000 (0.006)\tLoss 0.9062 (0.8930)\tTop 1-err 93.7500 (89.1827)\tTop 5-err 50.0000 (49.7596)\n",
            "Epoch: [2/5][50/1563]\tLR: 0.100000\tTime 0.362 (0.366)\tData 0.000 (0.003)\tLoss 0.9375 (0.8971)\tTop 1-err 90.6250 (89.6446)\tTop 5-err 46.8750 (49.3873)\n",
            "Epoch: [2/5][75/1563]\tLR: 0.100000\tTime 0.349 (0.363)\tData 0.000 (0.002)\tLoss 0.9062 (0.9009)\tTop 1-err 100.0000 (89.7615)\tTop 5-err 56.2500 (49.0954)\n",
            "Epoch: [2/5][100/1563]\tLR: 0.100000\tTime 0.352 (0.362)\tData 0.000 (0.002)\tLoss 0.8438 (0.9001)\tTop 1-err 90.6250 (90.0062)\tTop 5-err 46.8750 (48.7005)\n",
            "Epoch: [2/5][125/1563]\tLR: 0.100000\tTime 0.347 (0.361)\tData 0.000 (0.001)\tLoss 0.9062 (0.9000)\tTop 1-err 78.1250 (89.8065)\tTop 5-err 28.1250 (48.7351)\n",
            "Epoch: [2/5][150/1563]\tLR: 0.100000\tTime 0.354 (0.361)\tData 0.000 (0.001)\tLoss 0.8750 (0.9002)\tTop 1-err 87.5000 (89.8179)\tTop 5-err 43.7500 (48.2202)\n",
            "Epoch: [2/5][175/1563]\tLR: 0.100000\tTime 0.361 (0.360)\tData 0.000 (0.001)\tLoss 1.0000 (0.9034)\tTop 1-err 96.8750 (89.5774)\tTop 5-err 43.7500 (48.0646)\n",
            "Epoch: [2/5][200/1563]\tLR: 0.100000\tTime 0.356 (0.361)\tData 0.000 (0.001)\tLoss 0.9062 (0.9045)\tTop 1-err 93.7500 (89.6922)\tTop 5-err 68.7500 (48.4453)\n",
            "Epoch: [2/5][225/1563]\tLR: 0.100000\tTime 0.353 (0.360)\tData 0.000 (0.001)\tLoss 0.7500 (0.9040)\tTop 1-err 90.6250 (89.8092)\tTop 5-err 56.2500 (48.8385)\n",
            "Epoch: [2/5][250/1563]\tLR: 0.100000\tTime 0.357 (0.360)\tData 0.000 (0.001)\tLoss 0.8125 (0.9026)\tTop 1-err 78.1250 (89.8282)\tTop 5-err 34.3750 (48.6554)\n",
            "Epoch: [2/5][275/1563]\tLR: 0.100000\tTime 0.363 (0.359)\tData 0.000 (0.001)\tLoss 0.8438 (0.9013)\tTop 1-err 84.3750 (89.9343)\tTop 5-err 46.8750 (48.7319)\n",
            "Epoch: [2/5][300/1563]\tLR: 0.100000\tTime 0.346 (0.359)\tData 0.000 (0.001)\tLoss 0.8438 (0.9033)\tTop 1-err 90.6250 (90.1682)\tTop 5-err 37.5000 (48.9203)\n",
            "Epoch: [2/5][325/1563]\tLR: 0.100000\tTime 0.350 (0.359)\tData 0.000 (0.001)\tLoss 0.8750 (0.9028)\tTop 1-err 93.7500 (90.2032)\tTop 5-err 59.3750 (49.0318)\n",
            "Epoch: [2/5][350/1563]\tLR: 0.100000\tTime 0.362 (0.359)\tData 0.000 (0.001)\tLoss 0.8750 (0.9036)\tTop 1-err 90.6250 (90.2422)\tTop 5-err 50.0000 (48.9672)\n",
            "Epoch: [2/5][375/1563]\tLR: 0.100000\tTime 0.365 (0.359)\tData 0.000 (0.001)\tLoss 0.9375 (0.9036)\tTop 1-err 90.6250 (90.2593)\tTop 5-err 43.7500 (48.9943)\n",
            "Epoch: [2/5][400/1563]\tLR: 0.100000\tTime 0.374 (0.359)\tData 0.000 (0.001)\tLoss 0.9688 (0.9039)\tTop 1-err 96.8750 (90.2509)\tTop 5-err 56.2500 (49.0882)\n",
            "Epoch: [2/5][425/1563]\tLR: 0.100000\tTime 0.367 (0.360)\tData 0.000 (0.001)\tLoss 0.9375 (0.9035)\tTop 1-err 87.5000 (90.2582)\tTop 5-err 53.1250 (49.0830)\n",
            "Epoch: [2/5][450/1563]\tLR: 0.100000\tTime 0.348 (0.360)\tData 0.000 (0.000)\tLoss 0.9688 (0.9040)\tTop 1-err 90.6250 (90.2300)\tTop 5-err 37.5000 (49.0646)\n",
            "Epoch: [2/5][475/1563]\tLR: 0.100000\tTime 0.349 (0.359)\tData 0.000 (0.000)\tLoss 0.9688 (0.9042)\tTop 1-err 93.7500 (90.2114)\tTop 5-err 46.8750 (49.0809)\n",
            "Epoch: [2/5][500/1563]\tLR: 0.100000\tTime 0.360 (0.359)\tData 0.000 (0.000)\tLoss 1.0000 (0.9043)\tTop 1-err 90.6250 (90.2133)\tTop 5-err 59.3750 (49.1766)\n",
            "Epoch: [2/5][525/1563]\tLR: 0.100000\tTime 0.365 (0.359)\tData 0.000 (0.000)\tLoss 0.9062 (0.9042)\tTop 1-err 93.7500 (90.2091)\tTop 5-err 43.7500 (49.1980)\n",
            "Epoch: [2/5][550/1563]\tLR: 0.100000\tTime 0.355 (0.359)\tData 0.000 (0.000)\tLoss 0.8750 (0.9034)\tTop 1-err 87.5000 (90.1713)\tTop 5-err 34.3750 (49.1493)\n",
            "Epoch: [2/5][575/1563]\tLR: 0.100000\tTime 0.362 (0.359)\tData 0.000 (0.000)\tLoss 0.8750 (0.9037)\tTop 1-err 90.6250 (90.2181)\tTop 5-err 34.3750 (49.1916)\n",
            "Epoch: [2/5][600/1563]\tLR: 0.100000\tTime 0.359 (0.359)\tData 0.000 (0.000)\tLoss 0.9375 (0.9031)\tTop 1-err 84.3750 (90.2194)\tTop 5-err 46.8750 (49.2304)\n",
            "Epoch: [2/5][625/1563]\tLR: 0.100000\tTime 0.370 (0.360)\tData 0.000 (0.000)\tLoss 0.9375 (0.9026)\tTop 1-err 87.5000 (90.2007)\tTop 5-err 56.2500 (49.2163)\n",
            "Epoch: [2/5][650/1563]\tLR: 0.100000\tTime 0.371 (0.360)\tData 0.000 (0.000)\tLoss 0.8750 (0.9026)\tTop 1-err 90.6250 (90.1930)\tTop 5-err 46.8750 (49.1311)\n",
            "Epoch: [2/5][675/1563]\tLR: 0.100000\tTime 0.356 (0.360)\tData 0.000 (0.000)\tLoss 0.8125 (0.9023)\tTop 1-err 81.2500 (90.1581)\tTop 5-err 50.0000 (49.1679)\n",
            "Epoch: [2/5][700/1563]\tLR: 0.100000\tTime 0.351 (0.360)\tData 0.000 (0.000)\tLoss 0.9375 (0.9019)\tTop 1-err 78.1250 (90.1391)\tTop 5-err 37.5000 (49.1887)\n",
            "Epoch: [2/5][725/1563]\tLR: 0.100000\tTime 0.350 (0.359)\tData 0.000 (0.000)\tLoss 0.8438 (0.9016)\tTop 1-err 87.5000 (90.0956)\tTop 5-err 37.5000 (49.1262)\n",
            "Epoch: [2/5][750/1563]\tLR: 0.100000\tTime 0.357 (0.360)\tData 0.000 (0.000)\tLoss 0.9062 (0.9015)\tTop 1-err 84.3750 (90.0924)\tTop 5-err 50.0000 (49.2094)\n",
            "Epoch: [2/5][775/1563]\tLR: 0.100000\tTime 0.362 (0.359)\tData 0.000 (0.000)\tLoss 0.9062 (0.9016)\tTop 1-err 96.8750 (90.0894)\tTop 5-err 46.8750 (49.1543)\n",
            "Epoch: [2/5][800/1563]\tLR: 0.100000\tTime 0.351 (0.359)\tData 0.000 (0.000)\tLoss 0.9062 (0.9018)\tTop 1-err 90.6250 (90.0671)\tTop 5-err 40.6250 (49.2002)\n",
            "Epoch: [2/5][825/1563]\tLR: 0.100000\tTime 0.348 (0.359)\tData 0.000 (0.000)\tLoss 0.7812 (0.9015)\tTop 1-err 81.2500 (90.0575)\tTop 5-err 43.7500 (49.1942)\n",
            "Epoch: [2/5][850/1563]\tLR: 0.100000\tTime 0.366 (0.359)\tData 0.000 (0.000)\tLoss 0.9688 (0.9011)\tTop 1-err 87.5000 (90.0889)\tTop 5-err 50.0000 (49.1150)\n",
            "Epoch: [2/5][875/1563]\tLR: 0.100000\tTime 0.364 (0.359)\tData 0.000 (0.000)\tLoss 0.9062 (0.9009)\tTop 1-err 93.7500 (90.0899)\tTop 5-err 53.1250 (49.1117)\n",
            "Epoch: [2/5][900/1563]\tLR: 0.100000\tTime 0.359 (0.359)\tData 0.000 (0.000)\tLoss 0.8125 (0.9004)\tTop 1-err 90.6250 (90.0839)\tTop 5-err 50.0000 (49.1745)\n",
            "Epoch: [2/5][925/1563]\tLR: 0.100000\tTime 0.354 (0.359)\tData 0.000 (0.000)\tLoss 0.9375 (0.9005)\tTop 1-err 87.5000 (90.1154)\tTop 5-err 40.6250 (49.1226)\n",
            "Epoch: [2/5][950/1563]\tLR: 0.100000\tTime 0.353 (0.359)\tData 0.000 (0.000)\tLoss 0.9375 (0.9013)\tTop 1-err 93.7500 (90.0795)\tTop 5-err 50.0000 (49.1226)\n",
            "Epoch: [2/5][975/1563]\tLR: 0.100000\tTime 0.357 (0.359)\tData 0.000 (0.000)\tLoss 0.9375 (0.9012)\tTop 1-err 100.0000 (90.0903)\tTop 5-err 65.6250 (49.0747)\n",
            "Epoch: [2/5][1000/1563]\tLR: 0.100000\tTime 0.367 (0.359)\tData 0.000 (0.000)\tLoss 0.8438 (0.9011)\tTop 1-err 87.5000 (90.0849)\tTop 5-err 34.3750 (49.0759)\n",
            "Epoch: [2/5][1025/1563]\tLR: 0.100000\tTime 0.340 (0.359)\tData 0.000 (0.000)\tLoss 0.9375 (0.9013)\tTop 1-err 84.3750 (90.0737)\tTop 5-err 40.6250 (49.0101)\n",
            "Epoch: [2/5][1050/1563]\tLR: 0.100000\tTime 0.349 (0.359)\tData 0.000 (0.000)\tLoss 0.9375 (0.9008)\tTop 1-err 87.5000 (90.0838)\tTop 5-err 40.6250 (49.0188)\n",
            "Epoch: [2/5][1075/1563]\tLR: 0.100000\tTime 0.363 (0.359)\tData 0.000 (0.000)\tLoss 0.8750 (0.9010)\tTop 1-err 84.3750 (90.0848)\tTop 5-err 40.6250 (49.0154)\n",
            "Epoch: [2/5][1100/1563]\tLR: 0.100000\tTime 0.364 (0.359)\tData 0.000 (0.000)\tLoss 0.8750 (0.9007)\tTop 1-err 78.1250 (90.0829)\tTop 5-err 40.6250 (49.0293)\n",
            "Epoch: [2/5][1125/1563]\tLR: 0.100000\tTime 0.354 (0.359)\tData 0.000 (0.000)\tLoss 0.8438 (0.9007)\tTop 1-err 90.6250 (90.0727)\tTop 5-err 62.5000 (48.9759)\n",
            "Epoch: [2/5][1150/1563]\tLR: 0.100000\tTime 0.350 (0.359)\tData 0.000 (0.000)\tLoss 0.8750 (0.9006)\tTop 1-err 84.3750 (90.1146)\tTop 5-err 40.6250 (48.9846)\n",
            "Epoch: [2/5][1175/1563]\tLR: 0.100000\tTime 0.357 (0.359)\tData 0.000 (0.000)\tLoss 0.9062 (0.9008)\tTop 1-err 96.8750 (90.1095)\tTop 5-err 65.6250 (48.9876)\n",
            "Epoch: [2/5][1200/1563]\tLR: 0.100000\tTime 0.350 (0.358)\tData 0.000 (0.000)\tLoss 0.8750 (0.9009)\tTop 1-err 100.0000 (90.1644)\tTop 5-err 53.1250 (49.0034)\n",
            "Epoch: [2/5][1225/1563]\tLR: 0.100000\tTime 0.360 (0.358)\tData 0.000 (0.000)\tLoss 0.8750 (0.9009)\tTop 1-err 100.0000 (90.1738)\tTop 5-err 62.5000 (49.0161)\n",
            "Epoch: [2/5][1250/1563]\tLR: 0.100000\tTime 0.352 (0.358)\tData 0.000 (0.000)\tLoss 0.8438 (0.9010)\tTop 1-err 84.3750 (90.1829)\tTop 5-err 43.7500 (49.0158)\n",
            "Epoch: [2/5][1275/1563]\tLR: 0.100000\tTime 0.342 (0.358)\tData 0.000 (0.000)\tLoss 0.8438 (0.9011)\tTop 1-err 90.6250 (90.2111)\tTop 5-err 43.7500 (49.0669)\n",
            "Epoch: [2/5][1300/1563]\tLR: 0.100000\tTime 0.353 (0.358)\tData 0.000 (0.000)\tLoss 0.8438 (0.9011)\tTop 1-err 87.5000 (90.2094)\tTop 5-err 46.8750 (49.0224)\n",
            "Epoch: [2/5][1325/1563]\tLR: 0.100000\tTime 0.356 (0.358)\tData 0.000 (0.000)\tLoss 0.8438 (0.9006)\tTop 1-err 93.7500 (90.2031)\tTop 5-err 50.0000 (49.0220)\n",
            "Epoch: [2/5][1350/1563]\tLR: 0.100000\tTime 0.342 (0.358)\tData 0.000 (0.000)\tLoss 0.8750 (0.9004)\tTop 1-err 100.0000 (90.2364)\tTop 5-err 53.1250 (49.0123)\n",
            "Epoch: [2/5][1375/1563]\tLR: 0.100000\tTime 0.352 (0.358)\tData 0.000 (0.000)\tLoss 0.9062 (0.9003)\tTop 1-err 87.5000 (90.2230)\tTop 5-err 34.3750 (48.9644)\n",
            "Epoch: [2/5][1400/1563]\tLR: 0.100000\tTime 0.359 (0.358)\tData 0.000 (0.000)\tLoss 0.8438 (0.9002)\tTop 1-err 93.7500 (90.2257)\tTop 5-err 62.5000 (48.9940)\n",
            "Epoch: [2/5][1425/1563]\tLR: 0.100000\tTime 0.359 (0.358)\tData 0.000 (0.000)\tLoss 0.9688 (0.9003)\tTop 1-err 93.7500 (90.2393)\tTop 5-err 59.3750 (49.0095)\n",
            "Epoch: [2/5][1450/1563]\tLR: 0.100000\tTime 0.341 (0.358)\tData 0.000 (0.000)\tLoss 0.9062 (0.9001)\tTop 1-err 90.6250 (90.2610)\tTop 5-err 62.5000 (49.0136)\n",
            "Epoch: [2/5][1475/1563]\tLR: 0.100000\tTime 0.359 (0.358)\tData 0.000 (0.000)\tLoss 0.8125 (0.9000)\tTop 1-err 90.6250 (90.2693)\tTop 5-err 50.0000 (48.9880)\n",
            "Epoch: [2/5][1500/1563]\tLR: 0.100000\tTime 0.348 (0.358)\tData 0.000 (0.000)\tLoss 0.9375 (0.9001)\tTop 1-err 93.7500 (90.2752)\tTop 5-err 53.1250 (48.9819)\n",
            "Epoch: [2/5][1525/1563]\tLR: 0.100000\tTime 0.352 (0.358)\tData 0.000 (0.000)\tLoss 0.9375 (0.9000)\tTop 1-err 93.7500 (90.2830)\tTop 5-err 40.6250 (48.9986)\n",
            "Epoch: [2/5][1550/1563]\tLR: 0.100000\tTime 0.366 (0.358)\tData 0.000 (0.000)\tLoss 0.8750 (0.8998)\tTop 1-err 96.8750 (90.2805)\tTop 5-err 53.1250 (48.9523)\n",
            "Test (on val set): [2/5][0/313]\tTime 0.380 (0.380)\tLoss 0.9062 (0.9062)\tTop 1-err 93.7500 (93.7500)\tTop 5-err 50.0000 (50.0000)\n",
            "Test (on val set): [2/5][25/313]\tTime 0.223 (0.230)\tLoss 0.9062 (0.9087)\tTop 1-err 84.3750 (90.5048)\tTop 5-err 40.6250 (47.2356)\n",
            "Test (on val set): [2/5][50/313]\tTime 0.218 (0.227)\tLoss 0.9062 (0.9112)\tTop 1-err 96.8750 (90.6863)\tTop 5-err 56.2500 (48.4681)\n",
            "Test (on val set): [2/5][75/313]\tTime 0.221 (0.225)\tLoss 0.8438 (0.9112)\tTop 1-err 93.7500 (90.2961)\tTop 5-err 37.5000 (46.1760)\n",
            "Test (on val set): [2/5][100/313]\tTime 0.218 (0.224)\tLoss 0.9375 (0.9146)\tTop 1-err 87.5000 (90.5941)\tTop 5-err 31.2500 (46.3490)\n",
            "Test (on val set): [2/5][125/313]\tTime 0.222 (0.224)\tLoss 0.9062 (0.9115)\tTop 1-err 93.7500 (90.8730)\tTop 5-err 46.8750 (46.5030)\n",
            "Test (on val set): [2/5][150/313]\tTime 0.218 (0.224)\tLoss 0.9688 (0.9091)\tTop 1-err 81.2500 (90.6457)\tTop 5-err 43.7500 (46.4611)\n",
            "Test (on val set): [2/5][175/313]\tTime 0.219 (0.223)\tLoss 0.8750 (0.9064)\tTop 1-err 87.5000 (90.7848)\tTop 5-err 37.5000 (46.3778)\n",
            "Test (on val set): [2/5][200/313]\tTime 0.224 (0.223)\tLoss 0.9375 (0.9055)\tTop 1-err 87.5000 (90.8427)\tTop 5-err 40.6250 (46.7973)\n",
            "Test (on val set): [2/5][225/313]\tTime 0.219 (0.223)\tLoss 0.7812 (0.9057)\tTop 1-err 96.8750 (90.9016)\tTop 5-err 37.5000 (46.9856)\n",
            "Test (on val set): [2/5][250/313]\tTime 0.230 (0.223)\tLoss 0.8125 (0.9041)\tTop 1-err 93.7500 (90.9985)\tTop 5-err 56.2500 (47.0991)\n",
            "Test (on val set): [2/5][275/313]\tTime 0.216 (0.223)\tLoss 0.8750 (0.9034)\tTop 1-err 90.6250 (90.9873)\tTop 5-err 31.2500 (46.9769)\n",
            "Test (on val set): [2/5][300/313]\tTime 0.218 (0.223)\tLoss 0.9375 (0.9021)\tTop 1-err 90.6250 (90.9157)\tTop 5-err 53.1250 (47.1553)\n",
            "* Epoch: [2/5]\t Top 1-err 90.980  Top 5-err 47.240\t Test Loss 0.902\n",
            "Current best accuracy (top-1 and 5 error): tensor(89.9400, device='cuda:0') tensor(47.9600, device='cuda:0')\n",
            "Saved to /../../../content/gdrive/My Drive/PyramidNet/epistemic-checkpoint-epoch-2.pickle\n",
            "Epoch: [3/5][0/1563]\tLR: 0.010000\tTime 0.587 (0.587)\tData 0.148 (0.148)\tLoss 0.9062 (0.9062)\tTop 1-err 96.8750 (96.8750)\tTop 5-err 59.3750 (59.3750)\n",
            "Epoch: [3/5][25/1563]\tLR: 0.010000\tTime 0.344 (0.367)\tData 0.000 (0.006)\tLoss 0.9688 (0.9014)\tTop 1-err 100.0000 (90.3846)\tTop 5-err 75.0000 (50.1202)\n",
            "Epoch: [3/5][50/1563]\tLR: 0.010000\tTime 0.354 (0.361)\tData 0.000 (0.003)\tLoss 0.9062 (0.8940)\tTop 1-err 87.5000 (90.0123)\tTop 5-err 40.6250 (48.1005)\n",
            "Epoch: [3/5][75/1563]\tLR: 0.010000\tTime 0.349 (0.358)\tData 0.000 (0.002)\tLoss 0.8750 (0.8960)\tTop 1-err 90.6250 (89.5559)\tTop 5-err 46.8750 (48.1086)\n",
            "Epoch: [3/5][100/1563]\tLR: 0.010000\tTime 0.354 (0.357)\tData 0.000 (0.002)\tLoss 0.8750 (0.8963)\tTop 1-err 75.0000 (89.5730)\tTop 5-err 31.2500 (47.7723)\n",
            "Epoch: [3/5][125/1563]\tLR: 0.010000\tTime 0.354 (0.356)\tData 0.000 (0.001)\tLoss 0.8125 (0.8963)\tTop 1-err 84.3750 (89.4593)\tTop 5-err 40.6250 (48.0655)\n",
            "Epoch: [3/5][150/1563]\tLR: 0.010000\tTime 0.353 (0.356)\tData 0.000 (0.001)\tLoss 0.9375 (0.9011)\tTop 1-err 81.2500 (89.3005)\tTop 5-err 46.8750 (48.0546)\n",
            "Epoch: [3/5][175/1563]\tLR: 0.010000\tTime 0.349 (0.355)\tData 0.000 (0.001)\tLoss 0.8750 (0.9020)\tTop 1-err 90.6250 (89.2046)\tTop 5-err 56.2500 (47.8871)\n",
            "Epoch: [3/5][200/1563]\tLR: 0.010000\tTime 0.349 (0.355)\tData 0.000 (0.001)\tLoss 0.8750 (0.9010)\tTop 1-err 84.3750 (89.2413)\tTop 5-err 31.2500 (47.9633)\n",
            "Epoch: [3/5][225/1563]\tLR: 0.010000\tTime 0.360 (0.355)\tData 0.000 (0.001)\tLoss 0.9375 (0.9009)\tTop 1-err 90.6250 (89.4358)\tTop 5-err 53.1250 (48.2024)\n",
            "Epoch: [3/5][250/1563]\tLR: 0.010000\tTime 0.359 (0.355)\tData 0.000 (0.001)\tLoss 0.9062 (0.8990)\tTop 1-err 96.8750 (89.7535)\tTop 5-err 68.7500 (48.2570)\n",
            "Epoch: [3/5][275/1563]\tLR: 0.010000\tTime 0.358 (0.355)\tData 0.000 (0.001)\tLoss 0.9688 (0.9007)\tTop 1-err 87.5000 (89.6966)\tTop 5-err 40.6250 (48.0978)\n",
            "Epoch: [3/5][300/1563]\tLR: 0.010000\tTime 0.344 (0.354)\tData 0.000 (0.001)\tLoss 0.9375 (0.8993)\tTop 1-err 93.7500 (89.9502)\tTop 5-err 68.7500 (48.1831)\n",
            "Epoch: [3/5][325/1563]\tLR: 0.010000\tTime 0.344 (0.354)\tData 0.000 (0.001)\tLoss 0.9062 (0.8981)\tTop 1-err 81.2500 (89.8773)\tTop 5-err 59.3750 (48.1595)\n",
            "Epoch: [3/5][350/1563]\tLR: 0.010000\tTime 0.359 (0.354)\tData 0.000 (0.001)\tLoss 0.9375 (0.8989)\tTop 1-err 87.5000 (90.0552)\tTop 5-err 34.3750 (48.1660)\n",
            "Epoch: [3/5][375/1563]\tLR: 0.010000\tTime 0.346 (0.354)\tData 0.000 (0.001)\tLoss 0.8750 (0.8996)\tTop 1-err 93.7500 (90.1430)\tTop 5-err 40.6250 (48.2879)\n",
            "Epoch: [3/5][400/1563]\tLR: 0.010000\tTime 0.356 (0.354)\tData 0.000 (0.000)\tLoss 0.9688 (0.8995)\tTop 1-err 90.6250 (90.1263)\tTop 5-err 40.6250 (48.1219)\n",
            "Epoch: [3/5][425/1563]\tLR: 0.010000\tTime 0.358 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.8983)\tTop 1-err 90.6250 (90.2362)\tTop 5-err 46.8750 (48.2394)\n",
            "Epoch: [3/5][450/1563]\tLR: 0.010000\tTime 0.365 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.8977)\tTop 1-err 90.6250 (90.1469)\tTop 5-err 59.3750 (48.2470)\n",
            "Epoch: [3/5][475/1563]\tLR: 0.010000\tTime 0.345 (0.355)\tData 0.000 (0.000)\tLoss 0.9688 (0.8984)\tTop 1-err 90.6250 (90.2048)\tTop 5-err 50.0000 (48.3653)\n",
            "Epoch: [3/5][500/1563]\tLR: 0.010000\tTime 0.358 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.8988)\tTop 1-err 90.6250 (90.1634)\tTop 5-err 56.2500 (48.4281)\n",
            "Epoch: [3/5][525/1563]\tLR: 0.010000\tTime 0.355 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.8994)\tTop 1-err 78.1250 (90.1735)\tTop 5-err 34.3750 (48.4731)\n",
            "Epoch: [3/5][550/1563]\tLR: 0.010000\tTime 0.355 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.8996)\tTop 1-err 90.6250 (90.1656)\tTop 5-err 50.0000 (48.4403)\n",
            "Epoch: [3/5][575/1563]\tLR: 0.010000\tTime 0.356 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.8990)\tTop 1-err 90.6250 (90.1747)\tTop 5-err 65.6250 (48.5189)\n",
            "Epoch: [3/5][600/1563]\tLR: 0.010000\tTime 0.356 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.8988)\tTop 1-err 84.3750 (90.2194)\tTop 5-err 53.1250 (48.6117)\n",
            "Epoch: [3/5][625/1563]\tLR: 0.010000\tTime 0.347 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.8990)\tTop 1-err 100.0000 (90.2406)\tTop 5-err 53.1250 (48.6671)\n",
            "Epoch: [3/5][650/1563]\tLR: 0.010000\tTime 0.356 (0.355)\tData 0.000 (0.000)\tLoss 0.9688 (0.8993)\tTop 1-err 96.8750 (90.2506)\tTop 5-err 43.7500 (48.6511)\n",
            "Epoch: [3/5][675/1563]\tLR: 0.010000\tTime 0.354 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.8995)\tTop 1-err 90.6250 (90.2875)\tTop 5-err 59.3750 (48.5438)\n",
            "Epoch: [3/5][700/1563]\tLR: 0.010000\tTime 0.365 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.8994)\tTop 1-err 93.7500 (90.3040)\tTop 5-err 56.2500 (48.5333)\n",
            "Epoch: [3/5][725/1563]\tLR: 0.010000\tTime 0.341 (0.355)\tData 0.000 (0.000)\tLoss 0.8438 (0.8996)\tTop 1-err 93.7500 (90.3280)\tTop 5-err 50.0000 (48.5623)\n",
            "Epoch: [3/5][750/1563]\tLR: 0.010000\tTime 0.350 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.9000)\tTop 1-err 93.7500 (90.3129)\tTop 5-err 31.2500 (48.5811)\n",
            "Epoch: [3/5][775/1563]\tLR: 0.010000\tTime 0.360 (0.355)\tData 0.000 (0.000)\tLoss 0.7812 (0.9001)\tTop 1-err 90.6250 (90.3431)\tTop 5-err 56.2500 (48.6268)\n",
            "Epoch: [3/5][800/1563]\tLR: 0.010000\tTime 0.345 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9007)\tTop 1-err 90.6250 (90.3792)\tTop 5-err 56.2500 (48.6891)\n",
            "Epoch: [3/5][825/1563]\tLR: 0.010000\tTime 0.366 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9003)\tTop 1-err 84.3750 (90.3640)\tTop 5-err 40.6250 (48.6607)\n",
            "Epoch: [3/5][850/1563]\tLR: 0.010000\tTime 0.341 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.9005)\tTop 1-err 81.2500 (90.3349)\tTop 5-err 53.1250 (48.6927)\n",
            "Epoch: [3/5][875/1563]\tLR: 0.010000\tTime 0.353 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9007)\tTop 1-err 93.7500 (90.3503)\tTop 5-err 46.8750 (48.7764)\n",
            "Epoch: [3/5][900/1563]\tLR: 0.010000\tTime 0.362 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9005)\tTop 1-err 90.6250 (90.3892)\tTop 5-err 50.0000 (48.7826)\n",
            "Epoch: [3/5][925/1563]\tLR: 0.010000\tTime 0.353 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9005)\tTop 1-err 90.6250 (90.4394)\tTop 5-err 40.6250 (48.8020)\n",
            "Epoch: [3/5][950/1563]\tLR: 0.010000\tTime 0.367 (0.355)\tData 0.000 (0.000)\tLoss 0.8438 (0.9008)\tTop 1-err 87.5000 (90.4607)\tTop 5-err 37.5000 (48.8203)\n",
            "Epoch: [3/5][975/1563]\tLR: 0.010000\tTime 0.356 (0.355)\tData 0.000 (0.000)\tLoss 0.8438 (0.9004)\tTop 1-err 87.5000 (90.4265)\tTop 5-err 68.7500 (48.8601)\n",
            "Epoch: [3/5][1000/1563]\tLR: 0.010000\tTime 0.381 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9005)\tTop 1-err 90.6250 (90.4283)\tTop 5-err 50.0000 (48.7669)\n",
            "Epoch: [3/5][1025/1563]\tLR: 0.010000\tTime 0.346 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9006)\tTop 1-err 87.5000 (90.4270)\tTop 5-err 56.2500 (48.8000)\n",
            "Epoch: [3/5][1050/1563]\tLR: 0.010000\tTime 0.345 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9007)\tTop 1-err 93.7500 (90.4615)\tTop 5-err 62.5000 (48.8463)\n",
            "Epoch: [3/5][1075/1563]\tLR: 0.010000\tTime 0.352 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9010)\tTop 1-err 87.5000 (90.4653)\tTop 5-err 50.0000 (48.8238)\n",
            "Epoch: [3/5][1100/1563]\tLR: 0.010000\tTime 0.346 (0.355)\tData 0.000 (0.000)\tLoss 0.9688 (0.9011)\tTop 1-err 93.7500 (90.5143)\tTop 5-err 46.8750 (48.8590)\n",
            "Epoch: [3/5][1125/1563]\tLR: 0.010000\tTime 0.364 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.9010)\tTop 1-err 90.6250 (90.5223)\tTop 5-err 56.2500 (48.8399)\n",
            "Epoch: [3/5][1150/1563]\tLR: 0.010000\tTime 0.365 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.9007)\tTop 1-err 100.0000 (90.5327)\tTop 5-err 53.1250 (48.7972)\n",
            "Epoch: [3/5][1175/1563]\tLR: 0.010000\tTime 0.349 (0.355)\tData 0.000 (0.000)\tLoss 0.9688 (0.9008)\tTop 1-err 93.7500 (90.5506)\tTop 5-err 62.5000 (48.8255)\n",
            "Epoch: [3/5][1200/1563]\tLR: 0.010000\tTime 0.344 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9009)\tTop 1-err 87.5000 (90.5287)\tTop 5-err 34.3750 (48.8005)\n",
            "Epoch: [3/5][1225/1563]\tLR: 0.010000\tTime 0.351 (0.355)\tData 0.000 (0.000)\tLoss 0.8125 (0.9006)\tTop 1-err 90.6250 (90.5179)\tTop 5-err 50.0000 (48.7816)\n",
            "Epoch: [3/5][1250/1563]\tLR: 0.010000\tTime 0.355 (0.354)\tData 0.000 (0.000)\tLoss 0.9688 (0.9008)\tTop 1-err 90.6250 (90.5451)\tTop 5-err 50.0000 (48.8359)\n",
            "Epoch: [3/5][1275/1563]\tLR: 0.010000\tTime 0.387 (0.354)\tData 0.000 (0.000)\tLoss 0.9375 (0.9008)\tTop 1-err 96.8750 (90.5099)\tTop 5-err 34.3750 (48.8147)\n",
            "Epoch: [3/5][1300/1563]\tLR: 0.010000\tTime 0.357 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9010)\tTop 1-err 96.8750 (90.4977)\tTop 5-err 46.8750 (48.8110)\n",
            "Epoch: [3/5][1325/1563]\tLR: 0.010000\tTime 0.344 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9011)\tTop 1-err 87.5000 (90.4907)\tTop 5-err 53.1250 (48.8122)\n",
            "Epoch: [3/5][1350/1563]\tLR: 0.010000\tTime 0.355 (0.354)\tData 0.000 (0.000)\tLoss 0.8750 (0.9010)\tTop 1-err 93.7500 (90.4585)\tTop 5-err 50.0000 (48.8180)\n",
            "Epoch: [3/5][1375/1563]\tLR: 0.010000\tTime 0.358 (0.355)\tData 0.000 (0.000)\tLoss 0.9688 (0.9012)\tTop 1-err 96.8750 (90.4774)\tTop 5-err 50.0000 (48.8395)\n",
            "Epoch: [3/5][1400/1563]\tLR: 0.010000\tTime 0.347 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9009)\tTop 1-err 96.8750 (90.5001)\tTop 5-err 56.2500 (48.9494)\n",
            "Epoch: [3/5][1425/1563]\tLR: 0.010000\tTime 0.341 (0.354)\tData 0.000 (0.000)\tLoss 0.9375 (0.9010)\tTop 1-err 96.8750 (90.5308)\tTop 5-err 56.2500 (48.9744)\n",
            "Epoch: [3/5][1450/1563]\tLR: 0.010000\tTime 0.350 (0.354)\tData 0.000 (0.000)\tLoss 0.8750 (0.9011)\tTop 1-err 87.5000 (90.5195)\tTop 5-err 43.7500 (48.9512)\n",
            "Epoch: [3/5][1475/1563]\tLR: 0.010000\tTime 0.353 (0.354)\tData 0.000 (0.000)\tLoss 0.9062 (0.9008)\tTop 1-err 87.5000 (90.5255)\tTop 5-err 40.6250 (48.9499)\n",
            "Epoch: [3/5][1500/1563]\tLR: 0.010000\tTime 0.346 (0.354)\tData 0.000 (0.000)\tLoss 0.9062 (0.9009)\tTop 1-err 96.8750 (90.5001)\tTop 5-err 62.5000 (48.9528)\n",
            "Epoch: [3/5][1525/1563]\tLR: 0.010000\tTime 0.356 (0.354)\tData 0.000 (0.000)\tLoss 0.9062 (0.9010)\tTop 1-err 90.6250 (90.5124)\tTop 5-err 50.0000 (48.9843)\n",
            "Epoch: [3/5][1550/1563]\tLR: 0.010000\tTime 0.349 (0.354)\tData 0.000 (0.000)\tLoss 0.8750 (0.9011)\tTop 1-err 93.7500 (90.5081)\tTop 5-err 56.2500 (48.9543)\n",
            "Test (on val set): [3/5][0/313]\tTime 0.389 (0.389)\tLoss 0.8750 (0.8750)\tTop 1-err 90.6250 (90.6250)\tTop 5-err 46.8750 (46.8750)\n",
            "Test (on val set): [3/5][25/313]\tTime 0.219 (0.228)\tLoss 0.9375 (0.9135)\tTop 1-err 87.5000 (90.2644)\tTop 5-err 53.1250 (46.1538)\n",
            "Test (on val set): [3/5][50/313]\tTime 0.230 (0.227)\tLoss 1.0000 (0.9038)\tTop 1-err 84.3750 (90.6863)\tTop 5-err 43.7500 (47.1814)\n",
            "Test (on val set): [3/5][75/313]\tTime 0.224 (0.227)\tLoss 0.9062 (0.9013)\tTop 1-err 93.7500 (91.2829)\tTop 5-err 53.1250 (48.8076)\n",
            "Test (on val set): [3/5][100/313]\tTime 0.233 (0.226)\tLoss 0.8438 (0.9025)\tTop 1-err 93.7500 (91.2438)\tTop 5-err 46.8750 (47.9579)\n",
            "Test (on val set): [3/5][125/313]\tTime 0.218 (0.225)\tLoss 0.9375 (0.9033)\tTop 1-err 87.5000 (91.0218)\tTop 5-err 50.0000 (48.1399)\n",
            "Test (on val set): [3/5][150/313]\tTime 0.236 (0.225)\tLoss 0.9688 (0.9036)\tTop 1-err 90.6250 (91.1217)\tTop 5-err 50.0000 (48.3651)\n",
            "Test (on val set): [3/5][175/313]\tTime 0.223 (0.225)\tLoss 0.8125 (0.9022)\tTop 1-err 90.6250 (91.0156)\tTop 5-err 50.0000 (47.7450)\n",
            "Test (on val set): [3/5][200/313]\tTime 0.228 (0.225)\tLoss 0.8750 (0.9014)\tTop 1-err 100.0000 (91.0914)\tTop 5-err 40.6250 (47.5902)\n",
            "Test (on val set): [3/5][225/313]\tTime 0.214 (0.224)\tLoss 0.9375 (0.9018)\tTop 1-err 87.5000 (91.1090)\tTop 5-err 43.7500 (47.1930)\n",
            "Test (on val set): [3/5][250/313]\tTime 0.225 (0.224)\tLoss 0.9375 (0.9030)\tTop 1-err 90.6250 (91.1230)\tTop 5-err 34.3750 (47.3481)\n",
            "Test (on val set): [3/5][275/313]\tTime 0.217 (0.224)\tLoss 0.9375 (0.9021)\tTop 1-err 87.5000 (91.0326)\tTop 5-err 46.8750 (47.3845)\n",
            "Test (on val set): [3/5][300/313]\tTime 0.223 (0.223)\tLoss 0.9375 (0.9011)\tTop 1-err 90.6250 (90.9780)\tTop 5-err 31.2500 (47.2176)\n",
            "* Epoch: [3/5]\t Top 1-err 91.010  Top 5-err 47.160\t Test Loss 0.901\n",
            "Current best accuracy (top-1 and 5 error): tensor(89.9400, device='cuda:0') tensor(47.9600, device='cuda:0')\n",
            "Saved to /../../../content/gdrive/My Drive/PyramidNet/epistemic-checkpoint-epoch-3.pickle\n",
            "Epoch: [4/5][0/1563]\tLR: 0.001000\tTime 0.584 (0.584)\tData 0.162 (0.162)\tLoss 0.9375 (0.9375)\tTop 1-err 93.7500 (93.7500)\tTop 5-err 53.1250 (53.1250)\n",
            "Epoch: [4/5][25/1563]\tLR: 0.001000\tTime 0.348 (0.366)\tData 0.000 (0.006)\tLoss 0.9375 (0.9279)\tTop 1-err 93.7500 (91.2260)\tTop 5-err 43.7500 (51.5625)\n",
            "Epoch: [4/5][50/1563]\tLR: 0.001000\tTime 0.349 (0.360)\tData 0.000 (0.003)\tLoss 0.9062 (0.9148)\tTop 1-err 90.6250 (91.7892)\tTop 5-err 37.5000 (50.6740)\n",
            "Epoch: [4/5][75/1563]\tLR: 0.001000\tTime 0.355 (0.359)\tData 0.000 (0.002)\tLoss 0.8750 (0.9075)\tTop 1-err 100.0000 (92.2697)\tTop 5-err 40.6250 (50.7812)\n",
            "Epoch: [4/5][100/1563]\tLR: 0.001000\tTime 0.343 (0.357)\tData 0.000 (0.002)\tLoss 0.7500 (0.9066)\tTop 1-err 87.5000 (91.5842)\tTop 5-err 46.8750 (50.5569)\n",
            "Epoch: [4/5][125/1563]\tLR: 0.001000\tTime 0.350 (0.357)\tData 0.000 (0.001)\tLoss 0.8438 (0.9072)\tTop 1-err 87.5000 (91.5923)\tTop 5-err 46.8750 (50.2728)\n",
            "Epoch: [4/5][150/1563]\tLR: 0.001000\tTime 0.347 (0.356)\tData 0.000 (0.001)\tLoss 0.7188 (0.9021)\tTop 1-err 90.6250 (91.3907)\tTop 5-err 46.8750 (49.9586)\n",
            "Epoch: [4/5][175/1563]\tLR: 0.001000\tTime 0.347 (0.355)\tData 0.000 (0.001)\tLoss 0.9688 (0.8999)\tTop 1-err 90.6250 (91.4240)\tTop 5-err 40.6250 (49.8935)\n",
            "Epoch: [4/5][200/1563]\tLR: 0.001000\tTime 0.348 (0.355)\tData 0.000 (0.001)\tLoss 0.9688 (0.9005)\tTop 1-err 81.2500 (91.2780)\tTop 5-err 37.5000 (50.0000)\n",
            "Epoch: [4/5][225/1563]\tLR: 0.001000\tTime 0.356 (0.354)\tData 0.000 (0.001)\tLoss 0.9062 (0.9004)\tTop 1-err 90.6250 (91.2057)\tTop 5-err 59.3750 (50.0000)\n",
            "Epoch: [4/5][250/1563]\tLR: 0.001000\tTime 0.355 (0.354)\tData 0.000 (0.001)\tLoss 0.8125 (0.9003)\tTop 1-err 93.7500 (91.2351)\tTop 5-err 53.1250 (49.8755)\n",
            "Epoch: [4/5][275/1563]\tLR: 0.001000\tTime 0.350 (0.354)\tData 0.000 (0.001)\tLoss 0.9062 (0.9010)\tTop 1-err 84.3750 (91.0213)\tTop 5-err 46.8750 (49.6603)\n",
            "Epoch: [4/5][300/1563]\tLR: 0.001000\tTime 0.355 (0.354)\tData 0.000 (0.001)\tLoss 0.8750 (0.9007)\tTop 1-err 93.7500 (90.9157)\tTop 5-err 53.1250 (49.7197)\n",
            "Epoch: [4/5][325/1563]\tLR: 0.001000\tTime 0.345 (0.354)\tData 0.000 (0.001)\tLoss 0.9062 (0.9020)\tTop 1-err 84.3750 (90.8455)\tTop 5-err 37.5000 (49.6645)\n",
            "Epoch: [4/5][350/1563]\tLR: 0.001000\tTime 0.346 (0.354)\tData 0.000 (0.001)\tLoss 0.8438 (0.9010)\tTop 1-err 84.3750 (90.8832)\tTop 5-err 43.7500 (49.4569)\n",
            "Epoch: [4/5][375/1563]\tLR: 0.001000\tTime 0.358 (0.354)\tData 0.000 (0.001)\tLoss 0.9062 (0.9018)\tTop 1-err 81.2500 (90.7829)\tTop 5-err 46.8750 (49.4431)\n",
            "Epoch: [4/5][400/1563]\tLR: 0.001000\tTime 0.375 (0.354)\tData 0.000 (0.001)\tLoss 1.0000 (0.9030)\tTop 1-err 84.3750 (90.7107)\tTop 5-err 37.5000 (49.3766)\n",
            "Epoch: [4/5][425/1563]\tLR: 0.001000\tTime 0.357 (0.355)\tData 0.000 (0.000)\tLoss 0.9688 (0.9023)\tTop 1-err 90.6250 (90.6764)\tTop 5-err 40.6250 (49.3325)\n",
            "Epoch: [4/5][450/1563]\tLR: 0.001000\tTime 0.349 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9026)\tTop 1-err 87.5000 (90.5973)\tTop 5-err 37.5000 (49.2309)\n",
            "Epoch: [4/5][475/1563]\tLR: 0.001000\tTime 0.347 (0.354)\tData 0.000 (0.000)\tLoss 0.7500 (0.9028)\tTop 1-err 87.5000 (90.5265)\tTop 5-err 40.6250 (49.2647)\n",
            "Epoch: [4/5][500/1563]\tLR: 0.001000\tTime 0.350 (0.354)\tData 0.000 (0.000)\tLoss 0.9062 (0.9018)\tTop 1-err 78.1250 (90.5003)\tTop 5-err 43.7500 (49.1579)\n",
            "Epoch: [4/5][525/1563]\tLR: 0.001000\tTime 0.364 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9016)\tTop 1-err 84.3750 (90.4824)\tTop 5-err 56.2500 (49.1623)\n",
            "Epoch: [4/5][550/1563]\tLR: 0.001000\tTime 0.357 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9018)\tTop 1-err 93.7500 (90.5059)\tTop 5-err 71.8750 (49.0869)\n",
            "Epoch: [4/5][575/1563]\tLR: 0.001000\tTime 0.356 (0.355)\tData 0.000 (0.000)\tLoss 0.8125 (0.9019)\tTop 1-err 93.7500 (90.5273)\tTop 5-err 40.6250 (49.2025)\n",
            "Epoch: [4/5][600/1563]\tLR: 0.001000\tTime 0.399 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.9018)\tTop 1-err 87.5000 (90.5470)\tTop 5-err 53.1250 (49.1733)\n",
            "Epoch: [4/5][625/1563]\tLR: 0.001000\tTime 0.367 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9019)\tTop 1-err 93.7500 (90.5651)\tTop 5-err 50.0000 (49.1314)\n",
            "Epoch: [4/5][650/1563]\tLR: 0.001000\tTime 0.356 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.9018)\tTop 1-err 90.6250 (90.5530)\tTop 5-err 43.7500 (49.1023)\n",
            "Epoch: [4/5][675/1563]\tLR: 0.001000\tTime 0.350 (0.355)\tData 0.000 (0.000)\tLoss 0.8125 (0.9020)\tTop 1-err 87.5000 (90.5372)\tTop 5-err 31.2500 (49.1402)\n",
            "Epoch: [4/5][700/1563]\tLR: 0.001000\tTime 0.360 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.9023)\tTop 1-err 87.5000 (90.5091)\tTop 5-err 28.1250 (49.0906)\n",
            "Epoch: [4/5][725/1563]\tLR: 0.001000\tTime 0.354 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9017)\tTop 1-err 93.7500 (90.4528)\tTop 5-err 46.8750 (48.9583)\n",
            "Epoch: [4/5][750/1563]\tLR: 0.001000\tTime 0.360 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9011)\tTop 1-err 81.2500 (90.4169)\tTop 5-err 34.3750 (48.9306)\n",
            "Epoch: [4/5][775/1563]\tLR: 0.001000\tTime 0.356 (0.355)\tData 0.000 (0.000)\tLoss 0.7812 (0.9012)\tTop 1-err 87.5000 (90.3753)\tTop 5-err 40.6250 (48.9328)\n",
            "Epoch: [4/5][800/1563]\tLR: 0.001000\tTime 0.359 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9013)\tTop 1-err 93.7500 (90.3714)\tTop 5-err 62.5000 (48.8803)\n",
            "Epoch: [4/5][825/1563]\tLR: 0.001000\tTime 0.346 (0.355)\tData 0.000 (0.000)\tLoss 0.8438 (0.9011)\tTop 1-err 100.0000 (90.4396)\tTop 5-err 56.2500 (48.9482)\n",
            "Epoch: [4/5][850/1563]\tLR: 0.001000\tTime 0.356 (0.355)\tData 0.000 (0.000)\tLoss 0.8125 (0.9014)\tTop 1-err 90.6250 (90.4194)\tTop 5-err 56.2500 (48.9351)\n",
            "Epoch: [4/5][875/1563]\tLR: 0.001000\tTime 0.352 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.9012)\tTop 1-err 84.3750 (90.3860)\tTop 5-err 46.8750 (48.9191)\n",
            "Epoch: [4/5][900/1563]\tLR: 0.001000\tTime 0.348 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9012)\tTop 1-err 87.5000 (90.4134)\tTop 5-err 59.3750 (48.9803)\n",
            "Epoch: [4/5][925/1563]\tLR: 0.001000\tTime 0.383 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.9012)\tTop 1-err 87.5000 (90.3854)\tTop 5-err 46.8750 (48.9943)\n",
            "Epoch: [4/5][950/1563]\tLR: 0.001000\tTime 0.363 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.9011)\tTop 1-err 93.7500 (90.3917)\tTop 5-err 37.5000 (48.9255)\n",
            "Epoch: [4/5][975/1563]\tLR: 0.001000\tTime 0.353 (0.355)\tData 0.000 (0.000)\tLoss 0.7500 (0.9012)\tTop 1-err 84.3750 (90.4137)\tTop 5-err 59.3750 (48.9498)\n",
            "Epoch: [4/5][1000/1563]\tLR: 0.001000\tTime 0.351 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.9014)\tTop 1-err 84.3750 (90.4283)\tTop 5-err 53.1250 (48.8980)\n",
            "Epoch: [4/5][1025/1563]\tLR: 0.001000\tTime 0.362 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.9013)\tTop 1-err 84.3750 (90.3935)\tTop 5-err 46.8750 (48.8974)\n",
            "Epoch: [4/5][1050/1563]\tLR: 0.001000\tTime 0.353 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9009)\tTop 1-err 84.3750 (90.3633)\tTop 5-err 37.5000 (48.9058)\n",
            "Epoch: [4/5][1075/1563]\tLR: 0.001000\tTime 0.351 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9012)\tTop 1-err 81.2500 (90.3636)\tTop 5-err 50.0000 (48.9196)\n",
            "Epoch: [4/5][1100/1563]\tLR: 0.001000\tTime 0.360 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9009)\tTop 1-err 90.6250 (90.3809)\tTop 5-err 34.3750 (48.9441)\n",
            "Epoch: [4/5][1125/1563]\tLR: 0.001000\tTime 0.359 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9013)\tTop 1-err 96.8750 (90.3863)\tTop 5-err 53.1250 (48.9426)\n",
            "Epoch: [4/5][1150/1563]\tLR: 0.001000\tTime 0.346 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9017)\tTop 1-err 84.3750 (90.3698)\tTop 5-err 34.3750 (48.9004)\n",
            "Epoch: [4/5][1175/1563]\tLR: 0.001000\tTime 0.351 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.9017)\tTop 1-err 96.8750 (90.4018)\tTop 5-err 50.0000 (48.9929)\n",
            "Epoch: [4/5][1200/1563]\tLR: 0.001000\tTime 0.354 (0.355)\tData 0.000 (0.000)\tLoss 0.8438 (0.9012)\tTop 1-err 100.0000 (90.4168)\tTop 5-err 53.1250 (48.9722)\n",
            "Epoch: [4/5][1225/1563]\tLR: 0.001000\tTime 0.341 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9010)\tTop 1-err 96.8750 (90.4007)\tTop 5-err 46.8750 (49.0008)\n",
            "Epoch: [4/5][1250/1563]\tLR: 0.001000\tTime 0.347 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9008)\tTop 1-err 87.5000 (90.3752)\tTop 5-err 56.2500 (49.0283)\n",
            "Epoch: [4/5][1275/1563]\tLR: 0.001000\tTime 0.363 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9005)\tTop 1-err 90.6250 (90.3727)\tTop 5-err 53.1250 (49.0449)\n",
            "Epoch: [4/5][1300/1563]\tLR: 0.001000\tTime 0.356 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9004)\tTop 1-err 71.8750 (90.3416)\tTop 5-err 37.5000 (49.0416)\n",
            "Epoch: [4/5][1325/1563]\tLR: 0.001000\tTime 0.352 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.8998)\tTop 1-err 90.6250 (90.3422)\tTop 5-err 37.5000 (49.0856)\n",
            "Epoch: [4/5][1350/1563]\tLR: 0.001000\tTime 0.346 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9000)\tTop 1-err 87.5000 (90.3521)\tTop 5-err 40.6250 (49.0401)\n",
            "Epoch: [4/5][1375/1563]\tLR: 0.001000\tTime 0.353 (0.355)\tData 0.000 (0.000)\tLoss 0.7812 (0.9000)\tTop 1-err 93.7500 (90.3661)\tTop 5-err 53.1250 (49.0257)\n",
            "Epoch: [4/5][1400/1563]\tLR: 0.001000\tTime 0.368 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9000)\tTop 1-err 93.7500 (90.3640)\tTop 5-err 50.0000 (48.9516)\n",
            "Epoch: [4/5][1425/1563]\tLR: 0.001000\tTime 0.355 (0.355)\tData 0.000 (0.000)\tLoss 0.8438 (0.9003)\tTop 1-err 87.5000 (90.4059)\tTop 5-err 37.5000 (48.9941)\n",
            "Epoch: [4/5][1450/1563]\tLR: 0.001000\tTime 0.356 (0.355)\tData 0.000 (0.000)\tLoss 0.9688 (0.9004)\tTop 1-err 93.7500 (90.3881)\tTop 5-err 59.3750 (48.9232)\n",
            "Epoch: [4/5][1475/1563]\tLR: 0.001000\tTime 0.357 (0.355)\tData 0.000 (0.000)\tLoss 1.0000 (0.9004)\tTop 1-err 87.5000 (90.3942)\tTop 5-err 53.1250 (48.9181)\n",
            "Epoch: [4/5][1500/1563]\tLR: 0.001000\tTime 0.344 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9001)\tTop 1-err 100.0000 (90.3564)\tTop 5-err 43.7500 (48.8903)\n",
            "Epoch: [4/5][1525/1563]\tLR: 0.001000\tTime 0.357 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.9001)\tTop 1-err 93.7500 (90.3936)\tTop 5-err 59.3750 (48.8921)\n",
            "Epoch: [4/5][1550/1563]\tLR: 0.001000\tTime 0.352 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9001)\tTop 1-err 87.5000 (90.3671)\tTop 5-err 46.8750 (48.8717)\n",
            "Test (on val set): [4/5][0/313]\tTime 0.380 (0.380)\tLoss 0.9375 (0.9375)\tTop 1-err 90.6250 (90.6250)\tTop 5-err 56.2500 (56.2500)\n",
            "Test (on val set): [4/5][25/313]\tTime 0.232 (0.231)\tLoss 0.9688 (0.9207)\tTop 1-err 84.3750 (91.8269)\tTop 5-err 46.8750 (49.8798)\n",
            "Test (on val set): [4/5][50/313]\tTime 0.219 (0.229)\tLoss 0.8750 (0.9167)\tTop 1-err 87.5000 (91.0539)\tTop 5-err 37.5000 (48.8358)\n",
            "Test (on val set): [4/5][75/313]\tTime 0.235 (0.228)\tLoss 0.8438 (0.9157)\tTop 1-err 93.7500 (91.1595)\tTop 5-err 43.7500 (48.7253)\n",
            "Test (on val set): [4/5][100/313]\tTime 0.220 (0.227)\tLoss 0.9062 (0.9137)\tTop 1-err 87.5000 (90.9035)\tTop 5-err 37.5000 (47.6485)\n",
            "Test (on val set): [4/5][125/313]\tTime 0.235 (0.227)\tLoss 0.9062 (0.9097)\tTop 1-err 87.5000 (90.8234)\tTop 5-err 43.7500 (47.5446)\n",
            "Test (on val set): [4/5][150/313]\tTime 0.225 (0.227)\tLoss 0.9375 (0.9079)\tTop 1-err 90.6250 (90.5836)\tTop 5-err 43.7500 (47.6200)\n",
            "Test (on val set): [4/5][175/313]\tTime 0.220 (0.227)\tLoss 0.9062 (0.9057)\tTop 1-err 90.6250 (90.8381)\tTop 5-err 50.0000 (47.3899)\n",
            "Test (on val set): [4/5][200/313]\tTime 0.219 (0.226)\tLoss 0.8750 (0.9045)\tTop 1-err 90.6250 (90.9048)\tTop 5-err 46.8750 (47.2481)\n",
            "Test (on val set): [4/5][225/313]\tTime 0.223 (0.226)\tLoss 0.9688 (0.9032)\tTop 1-err 87.5000 (90.8324)\tTop 5-err 53.1250 (47.1654)\n",
            "Test (on val set): [4/5][250/313]\tTime 0.227 (0.226)\tLoss 0.8750 (0.9014)\tTop 1-err 84.3750 (90.6375)\tTop 5-err 46.8750 (47.0991)\n",
            "Test (on val set): [4/5][275/313]\tTime 0.227 (0.225)\tLoss 0.9062 (0.9009)\tTop 1-err 93.7500 (90.7156)\tTop 5-err 53.1250 (47.1241)\n",
            "Test (on val set): [4/5][300/313]\tTime 0.230 (0.225)\tLoss 0.9375 (0.9011)\tTop 1-err 93.7500 (90.7600)\tTop 5-err 43.7500 (47.0411)\n",
            "* Epoch: [4/5]\t Top 1-err 90.780  Top 5-err 47.080\t Test Loss 0.901\n",
            "Current best accuracy (top-1 and 5 error): tensor(89.9400, device='cuda:0') tensor(47.9600, device='cuda:0')\n",
            "Saved to /../../../content/gdrive/My Drive/PyramidNet/epistemic-checkpoint-epoch-4.pickle\n",
            "Epoch: [5/5][0/1563]\tLR: 0.000100\tTime 0.575 (0.575)\tData 0.171 (0.171)\tLoss 0.9688 (0.9688)\tTop 1-err 96.8750 (96.8750)\tTop 5-err 43.7500 (43.7500)\n",
            "Epoch: [5/5][25/1563]\tLR: 0.000100\tTime 0.346 (0.370)\tData 0.000 (0.007)\tLoss 0.9688 (0.9135)\tTop 1-err 90.6250 (90.9856)\tTop 5-err 56.2500 (49.0385)\n",
            "Epoch: [5/5][50/1563]\tLR: 0.000100\tTime 0.350 (0.362)\tData 0.000 (0.003)\tLoss 0.9375 (0.9081)\tTop 1-err 90.6250 (91.2990)\tTop 5-err 53.1250 (48.4069)\n",
            "Epoch: [5/5][75/1563]\tLR: 0.000100\tTime 0.352 (0.359)\tData 0.000 (0.002)\tLoss 0.9375 (0.9075)\tTop 1-err 90.6250 (90.9539)\tTop 5-err 53.1250 (48.7253)\n",
            "Epoch: [5/5][100/1563]\tLR: 0.000100\tTime 0.359 (0.359)\tData 0.000 (0.002)\tLoss 0.7812 (0.9025)\tTop 1-err 68.7500 (90.3465)\tTop 5-err 25.0000 (48.6077)\n",
            "Epoch: [5/5][125/1563]\tLR: 0.000100\tTime 0.353 (0.359)\tData 0.000 (0.001)\tLoss 0.9688 (0.9067)\tTop 1-err 96.8750 (90.5506)\tTop 5-err 56.2500 (48.5615)\n",
            "Epoch: [5/5][150/1563]\tLR: 0.000100\tTime 0.350 (0.358)\tData 0.000 (0.001)\tLoss 0.9375 (0.9075)\tTop 1-err 93.7500 (90.3974)\tTop 5-err 59.3750 (48.6548)\n",
            "Epoch: [5/5][175/1563]\tLR: 0.000100\tTime 0.357 (0.357)\tData 0.000 (0.001)\tLoss 0.9062 (0.9075)\tTop 1-err 100.0000 (90.4652)\tTop 5-err 62.5000 (49.1300)\n",
            "Epoch: [5/5][200/1563]\tLR: 0.000100\tTime 0.346 (0.357)\tData 0.000 (0.001)\tLoss 0.8125 (0.9059)\tTop 1-err 90.6250 (90.4229)\tTop 5-err 40.6250 (49.4092)\n",
            "Epoch: [5/5][225/1563]\tLR: 0.000100\tTime 0.342 (0.356)\tData 0.000 (0.001)\tLoss 0.9688 (0.9062)\tTop 1-err 87.5000 (90.3899)\tTop 5-err 59.3750 (49.2118)\n",
            "Epoch: [5/5][250/1563]\tLR: 0.000100\tTime 0.351 (0.356)\tData 0.000 (0.001)\tLoss 0.9375 (0.9059)\tTop 1-err 100.0000 (90.4258)\tTop 5-err 56.2500 (49.4646)\n",
            "Epoch: [5/5][275/1563]\tLR: 0.000100\tTime 0.346 (0.355)\tData 0.000 (0.001)\tLoss 0.8750 (0.9057)\tTop 1-err 90.6250 (90.4891)\tTop 5-err 46.8750 (49.2867)\n",
            "Epoch: [5/5][300/1563]\tLR: 0.000100\tTime 0.354 (0.355)\tData 0.000 (0.001)\tLoss 0.7812 (0.9039)\tTop 1-err 78.1250 (90.3862)\tTop 5-err 46.8750 (49.3667)\n",
            "Epoch: [5/5][325/1563]\tLR: 0.000100\tTime 0.363 (0.355)\tData 0.000 (0.001)\tLoss 0.8750 (0.9032)\tTop 1-err 87.5000 (90.5771)\tTop 5-err 50.0000 (49.4344)\n",
            "Epoch: [5/5][350/1563]\tLR: 0.000100\tTime 0.362 (0.355)\tData 0.000 (0.001)\tLoss 0.8750 (0.9030)\tTop 1-err 93.7500 (90.5449)\tTop 5-err 59.3750 (49.5816)\n",
            "Epoch: [5/5][375/1563]\tLR: 0.000100\tTime 0.361 (0.356)\tData 0.000 (0.001)\tLoss 0.9375 (0.9016)\tTop 1-err 90.6250 (90.3590)\tTop 5-err 34.3750 (49.3268)\n",
            "Epoch: [5/5][400/1563]\tLR: 0.000100\tTime 0.369 (0.356)\tData 0.000 (0.001)\tLoss 0.8750 (0.9020)\tTop 1-err 96.8750 (90.3445)\tTop 5-err 46.8750 (49.1895)\n",
            "Epoch: [5/5][425/1563]\tLR: 0.000100\tTime 0.348 (0.356)\tData 0.000 (0.001)\tLoss 0.8750 (0.9018)\tTop 1-err 93.7500 (90.2656)\tTop 5-err 53.1250 (49.0977)\n",
            "Epoch: [5/5][450/1563]\tLR: 0.000100\tTime 0.347 (0.356)\tData 0.000 (0.000)\tLoss 0.8438 (0.9015)\tTop 1-err 93.7500 (90.1885)\tTop 5-err 46.8750 (49.0161)\n",
            "Epoch: [5/5][475/1563]\tLR: 0.000100\tTime 0.356 (0.355)\tData 0.000 (0.000)\tLoss 0.8438 (0.9016)\tTop 1-err 90.6250 (90.1654)\tTop 5-err 34.3750 (48.9430)\n",
            "Epoch: [5/5][500/1563]\tLR: 0.000100\tTime 0.375 (0.356)\tData 0.000 (0.000)\tLoss 0.9375 (0.9023)\tTop 1-err 87.5000 (90.1572)\tTop 5-err 53.1250 (48.8273)\n",
            "Epoch: [5/5][525/1563]\tLR: 0.000100\tTime 0.346 (0.356)\tData 0.000 (0.000)\tLoss 0.8438 (0.9022)\tTop 1-err 90.6250 (90.1913)\tTop 5-err 34.3750 (48.9009)\n",
            "Epoch: [5/5][550/1563]\tLR: 0.000100\tTime 0.350 (0.356)\tData 0.000 (0.000)\tLoss 0.9375 (0.9025)\tTop 1-err 93.7500 (90.1259)\tTop 5-err 62.5000 (48.9054)\n",
            "Epoch: [5/5][575/1563]\tLR: 0.000100\tTime 0.368 (0.356)\tData 0.000 (0.000)\tLoss 0.8750 (0.9029)\tTop 1-err 84.3750 (90.1747)\tTop 5-err 34.3750 (48.8878)\n",
            "Epoch: [5/5][600/1563]\tLR: 0.000100\tTime 0.350 (0.356)\tData 0.000 (0.000)\tLoss 0.9062 (0.9025)\tTop 1-err 93.7500 (90.1830)\tTop 5-err 43.7500 (48.8457)\n",
            "Epoch: [5/5][625/1563]\tLR: 0.000100\tTime 0.348 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9028)\tTop 1-err 84.3750 (90.2306)\tTop 5-err 56.2500 (48.7670)\n",
            "Epoch: [5/5][650/1563]\tLR: 0.000100\tTime 0.347 (0.355)\tData 0.000 (0.000)\tLoss 0.8750 (0.9026)\tTop 1-err 87.5000 (90.2218)\tTop 5-err 46.8750 (48.7663)\n",
            "Epoch: [5/5][675/1563]\tLR: 0.000100\tTime 0.350 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9024)\tTop 1-err 93.7500 (90.2090)\tTop 5-err 59.3750 (48.7703)\n",
            "Epoch: [5/5][700/1563]\tLR: 0.000100\tTime 0.369 (0.355)\tData 0.000 (0.000)\tLoss 0.9688 (0.9025)\tTop 1-err 96.8750 (90.2104)\tTop 5-err 71.8750 (48.8053)\n",
            "Epoch: [5/5][725/1563]\tLR: 0.000100\tTime 0.365 (0.355)\tData 0.000 (0.000)\tLoss 0.9375 (0.9023)\tTop 1-err 96.8750 (90.2548)\tTop 5-err 43.7500 (48.8378)\n",
            "Epoch: [5/5][750/1563]\tLR: 0.000100\tTime 0.354 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9020)\tTop 1-err 87.5000 (90.2380)\tTop 5-err 40.6250 (48.7683)\n",
            "Epoch: [5/5][775/1563]\tLR: 0.000100\tTime 0.347 (0.355)\tData 0.000 (0.000)\tLoss 0.9688 (0.9020)\tTop 1-err 84.3750 (90.1941)\tTop 5-err 53.1250 (48.7959)\n",
            "Epoch: [5/5][800/1563]\tLR: 0.000100\tTime 0.363 (0.355)\tData 0.000 (0.000)\tLoss 0.8438 (0.9020)\tTop 1-err 96.8750 (90.1568)\tTop 5-err 53.1250 (48.7672)\n",
            "Epoch: [5/5][825/1563]\tLR: 0.000100\tTime 0.347 (0.355)\tData 0.000 (0.000)\tLoss 0.9688 (0.9024)\tTop 1-err 84.3750 (90.1748)\tTop 5-err 46.8750 (48.7931)\n",
            "Epoch: [5/5][850/1563]\tLR: 0.000100\tTime 0.357 (0.355)\tData 0.000 (0.000)\tLoss 0.9062 (0.9025)\tTop 1-err 93.7500 (90.1954)\tTop 5-err 43.7500 (48.7662)\n",
            "Epoch: [5/5][875/1563]\tLR: 0.000100\tTime 0.351 (0.355)\tData 0.000 (0.000)\tLoss 0.7812 (0.9020)\tTop 1-err 84.3750 (90.2219)\tTop 5-err 50.0000 (48.8620)\n",
            "Epoch: [5/5][900/1563]\tLR: 0.000100\tTime 0.346 (0.355)\tData 0.000 (0.000)\tLoss 0.7500 (0.9017)\tTop 1-err 90.6250 (90.2435)\tTop 5-err 43.7500 (48.8624)\n",
            "Epoch: [5/5][925/1563]\tLR: 0.000100\tTime 0.340 (0.355)\tData 0.000 (0.000)\tLoss 0.7812 (0.9020)\tTop 1-err 81.2500 (90.2504)\tTop 5-err 53.1250 (48.8897)\n",
            "Epoch: [5/5][950/1563]\tLR: 0.000100\tTime 0.348 (0.355)\tData 0.000 (0.000)\tLoss 0.8438 (0.9017)\tTop 1-err 93.7500 (90.2438)\tTop 5-err 50.0000 (48.9320)\n",
            "Epoch: [5/5][975/1563]\tLR: 0.000100\tTime 0.345 (0.354)\tData 0.000 (0.000)\tLoss 0.8750 (0.9019)\tTop 1-err 90.6250 (90.2440)\tTop 5-err 56.2500 (48.9018)\n",
            "Epoch: [5/5][1000/1563]\tLR: 0.000100\tTime 0.351 (0.354)\tData 0.000 (0.000)\tLoss 0.8750 (0.9018)\tTop 1-err 96.8750 (90.2504)\tTop 5-err 46.8750 (48.9292)\n",
            "Epoch: [5/5][1025/1563]\tLR: 0.000100\tTime 0.348 (0.354)\tData 0.000 (0.000)\tLoss 0.8750 (0.9021)\tTop 1-err 90.6250 (90.2961)\tTop 5-err 59.3750 (48.9279)\n",
            "Epoch: [5/5][1050/1563]\tLR: 0.000100\tTime 0.338 (0.354)\tData 0.000 (0.000)\tLoss 0.9375 (0.9020)\tTop 1-err 93.7500 (90.3187)\tTop 5-err 37.5000 (48.9534)\n",
            "Epoch: [5/5][1075/1563]\tLR: 0.000100\tTime 0.340 (0.354)\tData 0.000 (0.000)\tLoss 0.8750 (0.9017)\tTop 1-err 84.3750 (90.3230)\tTop 5-err 50.0000 (48.9196)\n",
            "Epoch: [5/5][1100/1563]\tLR: 0.000100\tTime 0.339 (0.354)\tData 0.000 (0.000)\tLoss 0.9062 (0.9017)\tTop 1-err 90.6250 (90.3412)\tTop 5-err 50.0000 (49.0066)\n",
            "Epoch: [5/5][1125/1563]\tLR: 0.000100\tTime 0.355 (0.353)\tData 0.000 (0.000)\tLoss 0.8125 (0.9015)\tTop 1-err 87.5000 (90.3225)\tTop 5-err 46.8750 (49.0425)\n",
            "Epoch: [5/5][1150/1563]\tLR: 0.000100\tTime 0.346 (0.353)\tData 0.000 (0.000)\tLoss 0.9062 (0.9015)\tTop 1-err 90.6250 (90.3263)\tTop 5-err 53.1250 (48.9900)\n",
            "Epoch: [5/5][1175/1563]\tLR: 0.000100\tTime 0.350 (0.353)\tData 0.000 (0.000)\tLoss 0.9062 (0.9015)\tTop 1-err 93.7500 (90.2902)\tTop 5-err 21.8750 (48.9450)\n",
            "Epoch: [5/5][1200/1563]\tLR: 0.000100\tTime 0.340 (0.353)\tData 0.000 (0.000)\tLoss 0.8750 (0.9015)\tTop 1-err 100.0000 (90.3024)\tTop 5-err 40.6250 (48.9228)\n",
            "Epoch: [5/5][1225/1563]\tLR: 0.000100\tTime 0.361 (0.353)\tData 0.000 (0.000)\tLoss 0.8750 (0.9017)\tTop 1-err 93.7500 (90.2656)\tTop 5-err 53.1250 (48.8836)\n",
            "Epoch: [5/5][1250/1563]\tLR: 0.000100\tTime 0.350 (0.353)\tData 0.000 (0.000)\tLoss 0.8750 (0.9013)\tTop 1-err 93.7500 (90.2753)\tTop 5-err 43.7500 (48.8409)\n",
            "Epoch: [5/5][1275/1563]\tLR: 0.000100\tTime 0.348 (0.353)\tData 0.000 (0.000)\tLoss 0.8750 (0.9012)\tTop 1-err 90.6250 (90.3164)\tTop 5-err 59.3750 (48.8171)\n",
            "Epoch: [5/5][1300/1563]\tLR: 0.000100\tTime 0.341 (0.353)\tData 0.000 (0.000)\tLoss 0.8750 (0.9008)\tTop 1-err 93.7500 (90.3127)\tTop 5-err 31.2500 (48.7774)\n",
            "Epoch: [5/5][1325/1563]\tLR: 0.000100\tTime 0.357 (0.352)\tData 0.000 (0.000)\tLoss 1.0000 (0.9008)\tTop 1-err 93.7500 (90.3375)\tTop 5-err 40.6250 (48.7698)\n",
            "Epoch: [5/5][1350/1563]\tLR: 0.000100\tTime 0.346 (0.352)\tData 0.000 (0.000)\tLoss 0.8750 (0.9007)\tTop 1-err 90.6250 (90.3289)\tTop 5-err 43.7500 (48.7833)\n",
            "Epoch: [5/5][1375/1563]\tLR: 0.000100\tTime 0.351 (0.352)\tData 0.000 (0.000)\tLoss 0.8750 (0.9003)\tTop 1-err 81.2500 (90.3525)\tTop 5-err 53.1250 (48.7804)\n",
            "Epoch: [5/5][1400/1563]\tLR: 0.000100\tTime 0.340 (0.352)\tData 0.000 (0.000)\tLoss 0.9375 (0.9000)\tTop 1-err 87.5000 (90.3484)\tTop 5-err 43.7500 (48.7888)\n",
            "Epoch: [5/5][1425/1563]\tLR: 0.000100\tTime 0.341 (0.352)\tData 0.000 (0.000)\tLoss 0.9375 (0.9002)\tTop 1-err 93.7500 (90.3226)\tTop 5-err 56.2500 (48.8166)\n",
            "Epoch: [5/5][1450/1563]\tLR: 0.000100\tTime 0.347 (0.352)\tData 0.000 (0.000)\tLoss 0.8438 (0.9003)\tTop 1-err 93.7500 (90.3364)\tTop 5-err 56.2500 (48.8069)\n",
            "Epoch: [5/5][1475/1563]\tLR: 0.000100\tTime 0.355 (0.352)\tData 0.000 (0.000)\tLoss 0.8750 (0.9002)\tTop 1-err 90.6250 (90.3413)\tTop 5-err 46.8750 (48.7995)\n",
            "Epoch: [5/5][1500/1563]\tLR: 0.000100\tTime 0.360 (0.352)\tData 0.000 (0.000)\tLoss 0.8750 (0.9003)\tTop 1-err 78.1250 (90.3273)\tTop 5-err 50.0000 (48.8362)\n",
            "Epoch: [5/5][1525/1563]\tLR: 0.000100\tTime 0.343 (0.352)\tData 0.000 (0.000)\tLoss 0.9375 (0.9004)\tTop 1-err 90.6250 (90.3465)\tTop 5-err 53.1250 (48.8389)\n",
            "Epoch: [5/5][1550/1563]\tLR: 0.000100\tTime 0.350 (0.352)\tData 0.000 (0.000)\tLoss 0.9688 (0.9004)\tTop 1-err 93.7500 (90.3812)\tTop 5-err 62.5000 (48.8495)\n",
            "Test (on val set): [5/5][0/313]\tTime 0.373 (0.373)\tLoss 0.9688 (0.9688)\tTop 1-err 87.5000 (87.5000)\tTop 5-err 46.8750 (46.8750)\n",
            "Test (on val set): [5/5][25/313]\tTime 0.229 (0.225)\tLoss 0.9375 (0.9159)\tTop 1-err 100.0000 (89.1827)\tTop 5-err 43.7500 (45.7933)\n",
            "Test (on val set): [5/5][50/313]\tTime 0.214 (0.222)\tLoss 0.8438 (0.9075)\tTop 1-err 90.6250 (90.0735)\tTop 5-err 37.5000 (45.7108)\n",
            "Test (on val set): [5/5][75/313]\tTime 0.215 (0.221)\tLoss 0.9062 (0.9038)\tTop 1-err 96.8750 (89.8026)\tTop 5-err 56.2500 (46.1760)\n",
            "Test (on val set): [5/5][100/313]\tTime 0.223 (0.221)\tLoss 0.9062 (0.9066)\tTop 1-err 84.3750 (89.5421)\tTop 5-err 34.3750 (46.8131)\n",
            "Test (on val set): [5/5][125/313]\tTime 0.215 (0.220)\tLoss 0.9375 (0.9075)\tTop 1-err 90.6250 (89.6825)\tTop 5-err 40.6250 (47.0982)\n",
            "Test (on val set): [5/5][150/313]\tTime 0.219 (0.220)\tLoss 0.8750 (0.9085)\tTop 1-err 96.8750 (89.6730)\tTop 5-err 53.1250 (47.3924)\n",
            "Test (on val set): [5/5][175/313]\tTime 0.223 (0.220)\tLoss 0.9062 (0.9066)\tTop 1-err 84.3750 (89.5241)\tTop 5-err 46.8750 (47.2656)\n",
            "Test (on val set): [5/5][200/313]\tTime 0.213 (0.220)\tLoss 0.9062 (0.9053)\tTop 1-err 96.8750 (89.3968)\tTop 5-err 50.0000 (47.1082)\n",
            "Test (on val set): [5/5][225/313]\tTime 0.220 (0.220)\tLoss 0.9062 (0.9029)\tTop 1-err 87.5000 (89.5326)\tTop 5-err 43.7500 (47.4558)\n",
            "Test (on val set): [5/5][250/313]\tTime 0.215 (0.220)\tLoss 0.9375 (0.9034)\tTop 1-err 90.6250 (89.4547)\tTop 5-err 46.8750 (47.4353)\n",
            "Test (on val set): [5/5][275/313]\tTime 0.217 (0.220)\tLoss 0.8125 (0.9027)\tTop 1-err 87.5000 (89.4928)\tTop 5-err 34.3750 (47.4524)\n",
            "Test (on val set): [5/5][300/313]\tTime 0.220 (0.220)\tLoss 0.9375 (0.9010)\tTop 1-err 78.1250 (89.3895)\tTop 5-err 46.8750 (47.2488)\n",
            "* Epoch: [5/5]\t Top 1-err 89.500  Top 5-err 47.280\t Test Loss 0.901\n",
            "Current best accuracy (top-1 and 5 error): tensor(89.5000, device='cuda:0') tensor(47.2800, device='cuda:0')\n",
            "Saved to /../../../content/gdrive/My Drive/PyramidNet/epistemic-checkpoint-epoch-5.pickle\n",
            "Best accuracy (top-1 and 5 error): tensor(89.5000, device='cuda:0') tensor(47.2800, device='cuda:0')\n",
            "Saved to /../../../content/gdrive/My Drive/PyramidNet/epistemic-results.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXev1yYyNkdo",
        "colab_type": "text"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pl3maUA1nAF",
        "colab_type": "text"
      },
      "source": [
        "Try plotting the Top 1-error and Top 5-error computed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jx1KQSQNmjF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "c19fb392-75b5-4013-899d-55f174bf7404"
      },
      "source": [
        "def plot_errors(results):\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(results[\"epoch\"], results[\"err1\"], \"o-\")\n",
        "  plt.title(\"Top 1-Error vs Epochs\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Top 1-Error (%)\")\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(results[\"epoch\"], results[\"err5\"], \"o-\")\n",
        "  plt.title(\"Top 5-Error vs Epochs\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Top 5-Error (%)\")\n",
        "  \n",
        "  # plt.gcf().set_size_inches(14, 4)\n",
        "  plt.show()\n",
        "\n",
        "results = load_from_pickle(\"/../../../content/gdrive/My Drive/PyramidNet/{}-results\".format(loss_func))\n",
        "plot_errors(results)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded from /../../../content/gdrive/My Drive/PyramidNet/epistemic-results.pickle\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kHi6guk6S3w",
        "colab_type": "text"
      },
      "source": [
        "##Simple Analysis:\n",
        "\n",
        "Since we are only observing the initial 5 epochs, which also lack learning rate decay (it decays every 30 epochs), we will not see much improvement on the accuracy. In fact, cross entropy loss function yields very similar results to the custom made epistemic loss function. \n",
        "\n",
        "However, this implementation provides a starting point to fine-tune and further train the model to yield better results with integration of epistemic uncertainty as classification loss function."
      ]
    }
  ]
}